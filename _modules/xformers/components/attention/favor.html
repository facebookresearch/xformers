


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>xformers.components.attention.favor | xFormers 0.0.29 documentation</title>
  
  <script src="../../../../_static/js/ga.js"></script>
  <script src="../../../../_static/js/redirect.js"></script>
  
  <link rel="shortcut icon" href="../../../../_static/images/favicon.png" />
  
  
  <link rel="canonical" href="https://facebookresearch.github.io/xformers_modules/xformers/components/attention/favor.html" />
  
  <meta property="og:title" content="xformers.components.attention.favor | xFormers 0.0.29 documentation">
  <meta name="description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <meta property="og:description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <!--<meta property="og:image" content="https://mmf.sh/img/logo.png">-->
  <!--<meta property="twitter:image" content="https://mmf.sh/img/logo.png">-->
  <meta name="twitter:image:alt" content="Image for xFormers">
  <meta name="twitter:card" content="summary_large_image">
  

  
  
  

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/css/customize.css" type="text/css" />
  <link rel="index" title="Index" href="../../../../genindex.html" />
  <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/facebookresearch/xformers"><img
          src="../../../../_static/logo.png" style="padding-right: 90px;" width="280px" height="53px"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers"> xFormers Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../what_is_xformers.html">What is xFormers?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Components Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../components/index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials and examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/index.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Some custom parts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../custom_parts/index.html">Custom parts reference</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../attention.html">xformers.components.attention</a> &gt;</li>
        
      <li>xformers.components.attention.favor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <h1>Source code for xformers.components.attention.favor</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.amp</span> <span class="kn">import</span> <span class="n">autocast</span>

<span class="kn">from</span> <span class="nn">xformers.components.attention</span> <span class="kn">import</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">AttentionConfig</span><span class="p">,</span> <span class="n">register_attention</span>
<span class="kn">from</span> <span class="nn">xformers.components.attention.feature_maps</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FeatureMap</span><span class="p">,</span>
    <span class="n">FeatureMapType</span><span class="p">,</span>
    <span class="n">SMHyperbolic</span><span class="p">,</span>
    <span class="n">SMOrf</span><span class="p">,</span>
    <span class="n">SMReg</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;xformers&quot;</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">FavorAttentionConfig</span><span class="p">(</span><span class="n">AttentionConfig</span><span class="p">):</span>
    <span class="n">causal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
    <span class="n">dim_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># The dimensions of the random features</span>
    <span class="n">dim_head</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="nb">int</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># The embedding dimension of the inputs. Only useful to get a dim_features estimate</span>
    <span class="n">iter_before_redraw</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="nb">int</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># The number of iterations before the random features are re-drawn from scratch</span>
    <span class="n">feature_map</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FeatureMapType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<div class="viewcode-block" id="FavorAttention"><a class="viewcode-back" href="../../../../components/attentions.html#xformers.components.attention.FavorAttention">[docs]</a><span class="nd">@register_attention</span><span class="p">(</span><span class="s2">&quot;favor&quot;</span><span class="p">,</span> <span class="n">FavorAttentionConfig</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">FavorAttention</span><span class="p">(</span><span class="n">Attention</span><span class="p">):</span>
<div class="viewcode-block" id="FavorAttention.__init__"><a class="viewcode-back" href="../../../../components/attentions.html#xformers.components.attention.FavorAttention.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">dim_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dim_head</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">iter_before_redraw</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">feature_map_type</span><span class="p">:</span> <span class="n">FeatureMapType</span> <span class="o">=</span> <span class="n">FeatureMapType</span><span class="o">.</span><span class="n">SMReg</span><span class="p">,</span>
        <span class="n">normalize_inputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="n">_</span><span class="p">,</span>
        <span class="o">**</span><span class="n">__</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Kernelized attention, as proposed in Performers_</span>
<span class="sd">        (&quot;Rethinking attention with performers.&quot; K. Choromanski et al. (2020).).</span>

<span class="sd">        FAVOR stands for &quot;Fast Attention Via positive Orthogonal Random features&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            dropout (float): the probability of an output to be randomly dropped at training time</span>
<span class="sd">            dim_features (int): the dimension of the random features space</span>
<span class="sd">            iter_before_redraw (int): the number of steps (forward calls) before a redraw of the features</span>
<span class="sd">            feature_map_type (FeatureMapType): the type of feature map being used,</span>
<span class="sd">            for instance orthogonal random features.</span>

<span class="sd">        .. _Performers: https://arxiv.org/pdf/2009.14794v1.pdf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter_before_redraw</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">iter_before_redraw</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">iter_before_redraw</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">iter_before_redraw</span>
        <span class="p">)</span>  <span class="c1"># This will be used for both key and query</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_inputs</span> <span class="o">=</span> <span class="n">normalize_inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_map_type</span> <span class="o">=</span> <span class="n">feature_map_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Setup dimension-dependent variables</span>
        <span class="c1"># Reasonable dimension default</span>
        <span class="k">if</span> <span class="n">dim_features</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">dim_head</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;dim_features or dim_head needs to be passed&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim_features</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">dim_head</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">dim_head</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim_features</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dim_features</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="p">)</span>  <span class="c1"># needs to be even for some variants</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;FAVOR: Automatically setting the random mapping dimension to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_features</span><span class="si">}</span><span class="s2"> from </span><span class="si">{</span><span class="n">dim_head</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim_features</span> <span class="o">=</span> <span class="n">dim_features</span>

        <span class="n">feature_map_constructor</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">FeatureMapType</span><span class="o">.</span><span class="n">SMHyp</span><span class="p">:</span> <span class="n">SMHyperbolic</span><span class="p">,</span>
            <span class="n">FeatureMapType</span><span class="o">.</span><span class="n">SMReg</span><span class="p">:</span> <span class="n">SMReg</span><span class="p">,</span>
            <span class="n">FeatureMapType</span><span class="o">.</span><span class="n">SMOrf</span><span class="p">:</span> <span class="n">SMOrf</span><span class="p">,</span>
        <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_map_type</span><span class="p">]</span>

        <span class="n">feature_settings</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;dim_features&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_features</span><span class="p">,</span>
            <span class="s2">&quot;iter_before_redraw&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_before_redraw</span><span class="p">,</span>
            <span class="s2">&quot;normalize_inputs&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_inputs</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">feature_map</span><span class="p">:</span> <span class="n">FeatureMap</span> <span class="o">=</span> <span class="n">feature_map_constructor</span><span class="p">(</span><span class="o">**</span><span class="n">feature_settings</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># Properties specific to this attention mechanism</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">supports_attention_mask</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">supports_key_padding_mask</span> <span class="o">=</span> <span class="kc">False</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_maybe_promote</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Only promote fp16 buffers, bfloat16 would be fine for instance</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">else</span> <span class="n">x</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_causal_attention</span><span class="p">(</span>
        <span class="n">k_prime</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">q_prime</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="c1"># Algorithm 1 in the paper</span>
        <span class="n">ref_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># BATCH x SEQ x 1 x EMB</span>
        <span class="n">Gps</span> <span class="o">=</span> <span class="n">k_prime</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">v</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">Grenorm</span> <span class="o">=</span> <span class="n">k_prime</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">ref_v</span>

        <span class="c1"># Consolidate against the feature dimension</span>
        <span class="n">att_raw</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bcfe,bcf-&gt;bce&quot;</span><span class="p">,</span> <span class="n">Gps</span><span class="p">,</span> <span class="n">q_prime</span><span class="p">)</span>
        <span class="n">att_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bcfe,bcf-&gt;bce&quot;</span><span class="p">,</span> <span class="n">Grenorm</span><span class="p">,</span> <span class="n">q_prime</span><span class="p">)</span>

        <span class="c1"># Cumulative sum over the sequence</span>
        <span class="n">att_raw</span> <span class="o">=</span> <span class="n">att_raw</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">att_norm</span> <span class="o">=</span> <span class="n">att_norm</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">att_raw</span><span class="p">,</span> <span class="n">att_norm</span>

<div class="viewcode-block" id="FavorAttention.forward"><a class="viewcode-back" href="../../../../components/attentions.html#xformers.components.attention.FavorAttention.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="o">*</span><span class="n">_</span><span class="p">,</span>
        <span class="o">**</span><span class="n">__</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="c1"># Project key and queries onto the feature map space</span>
        <span class="n">k_prime</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_map</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="n">q_prime</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_map</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="c1"># The softmax kernel approximation for Favor will easily overflow</span>
            <span class="c1"># Force the computations here to stay in fp32 for numerical stability</span>
            <span class="c1"># Note that the dimensions are vastly reduced when compared to scaled_dot_product</span>
            <span class="n">k_prime</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_promote</span><span class="p">(</span><span class="n">k_prime</span><span class="p">)</span>
            <span class="n">q_prime</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_promote</span><span class="p">(</span><span class="n">q_prime</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_promote</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span><span class="p">:</span>
                <span class="n">att_normalization</span> <span class="o">=</span> <span class="n">q_prime</span> <span class="o">@</span> <span class="p">(</span>
                    <span class="n">k_prime</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">att_raw</span> <span class="o">=</span> <span class="n">q_prime</span> <span class="o">@</span> <span class="p">(</span><span class="n">k_prime</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Actually compute attention</span>
                <span class="n">att_raw</span><span class="p">,</span> <span class="n">att_normalization</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_causal_attention</span><span class="p">(</span><span class="n">k_prime</span><span class="p">,</span> <span class="n">q_prime</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

            <span class="c1"># Normalize</span>
            <span class="n">att</span> <span class="o">=</span> <span class="n">att_raw</span> <span class="o">/</span> <span class="n">att_normalization</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">att</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">att</span></div></div>
</pre></div>

              </article>
              
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright © 2021 Meta Platforms, Inc.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <!-- <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div> -->
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../../../../"
    src="../../../../_static/documentation_options.js"></script>
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
  <script src="../../../../_static/jquery.js"></script>
  <script src="../../../../_static/underscore.js"></script>
  <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../../../../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/facebookresearch/xformers" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/facebookresearch/xformers">fairscale</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Resources</a></li>
            <li><a href="https://github.com/facebookresearch/xformers">Docs</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://opensource.facebook.com/legal/terms">Terms of Use</a></li>
            <li><a href="https://opensource.facebook.com/legal/privacy">Privacy Policy</a></li>
          </ul>
        </div>
      </div>
    </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/facebookresearch/xformers" aria-label="fairscale"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Docs</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>
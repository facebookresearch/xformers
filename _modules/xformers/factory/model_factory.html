


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>xformers.factory.model_factory | xFormers 0.0.21 documentation</title>
  
  <script src="../../../_static/js/ga.js"></script>
  <script src="../../../_static/js/redirect.js"></script>
  
  <link rel="shortcut icon" href="../../../_static/images/favicon.png" />
  
  
  <link rel="canonical" href="https://facebookresearch.github.io/xformers_modules/xformers/factory/model_factory.html" />
  
  <meta property="og:title" content="xformers.factory.model_factory | xFormers 0.0.21 documentation">
  <meta name="description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <meta property="og:description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <!--<meta property="og:image" content="https://mmf.sh/img/logo.png">-->
  <!--<meta property="twitter:image" content="https://mmf.sh/img/logo.png">-->
  <meta name="twitter:image:alt" content="Image for xFormers">
  <meta name="twitter:card" content="summary_large_image">
  

  
  
  

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/css/customize.css" type="text/css" />
  <link rel="index" title="Index" href="../../../genindex.html" />
  <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/facebookresearch/xformers"><img
          src="../../../_static/logo.png" style="padding-right: 90px;" width="280px" height="53px"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers"> xFormers Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../what_is_xformers.html">What is xFormers?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Components Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../components/index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build models and blocks programatically</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../factory/index.html">Factory</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials and examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Some custom parts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../custom_parts/index.html">Custom parts reference</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>xformers.factory.model_factory</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <h1>Source code for xformers.factory.model_factory</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>


<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">xformers.components</span> <span class="kn">import</span> <span class="n">reversible</span> <span class="k">as</span> <span class="n">rv</span>
<span class="kn">from</span> <span class="nn">xformers.components.residual</span> <span class="kn">import</span> <span class="n">ResidualNormStyle</span><span class="p">,</span> <span class="n">get_deepnorm_coefficients</span>
<span class="kn">from</span> <span class="nn">xformers.factory.block_configs</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">xFormerBlockConfig</span><span class="p">,</span>
    <span class="n">xFormerDecoderConfig</span><span class="p">,</span>
    <span class="n">xFormerEncoderConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">xformers.factory.block_factory</span> <span class="kn">import</span> <span class="n">xFormerDecoderBlock</span><span class="p">,</span> <span class="n">xFormerEncoderBlock</span>
<span class="kn">from</span> <span class="nn">xformers.factory.weight_init</span> <span class="kn">import</span> <span class="n">get_weight_init_fn</span><span class="p">,</span> <span class="n">xFormerWeightInit</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;xformers&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="xFormerConfig"><a class="viewcode-back" href="../../../factory/model.html#xformers.factory.model_factory.xFormerConfig">[docs]</a><span class="nd">@dataclass</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">xFormerConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The configuration structure to define a full Transformer.</span>
<span class="sd">    This can include a stack of encoder layers, and a stack of decoder layers.</span>

<span class="sd">    It is optionally possible to share the embedding weights in between</span>
<span class="sd">    the encoder and decoder positional encoding, as proposed for instance by</span>
<span class="sd">    `Using the Output Embedding to Improve Language Models`, Press et al.</span>

<span class="sd">    A full config example is for instance as follows:</span>

<span class="sd">    ::</span>

<span class="sd">        xformer_config = [</span>
<span class="sd">            {</span>
<span class="sd">                &quot;reversible&quot;: False,  # Turn on to test the effect of using reversible layers</span>
<span class="sd">                &quot;block_type&quot;: &quot;encoder&quot;,</span>
<span class="sd">                &quot;num_layers&quot;: LAYERS,</span>
<span class="sd">                &quot;dim_model&quot;: EMB,</span>
<span class="sd">                &quot;residual_norm_style&quot;: &quot;pre&quot;,</span>
<span class="sd">                &quot;position_encoding_config&quot;: {</span>
<span class="sd">                    &quot;name&quot;: &quot;vocab&quot;,</span>
<span class="sd">                    &quot;seq_len&quot;: CONTEXT,</span>
<span class="sd">                    &quot;vocab_size&quot;: VOCAB_SIZE,</span>
<span class="sd">                },</span>
<span class="sd">                &quot;multi_head_config&quot;: {</span>
<span class="sd">                    &quot;num_heads&quot;: NUM_HEADS,</span>
<span class="sd">                    &quot;residual_dropout&quot;: RES_DROP,</span>
<span class="sd">                    &quot;use_rotary_embeddings&quot;: True,</span>
<span class="sd">                    &quot;attention&quot;: {</span>
<span class="sd">                        &quot;name&quot;: ATTENTION_MECHANISM_STR,</span>
<span class="sd">                        &quot;dropout&quot;: ATTN_DROP,</span>
<span class="sd">                        &quot;causal&quot;: True,</span>
<span class="sd">                        &quot;seq_len&quot;: CONTEXT,</span>
<span class="sd">                    },</span>
<span class="sd">                },</span>
<span class="sd">                &quot;feedforward_config&quot;: {</span>
<span class="sd">                    &quot;name&quot;: &quot;FusedMLP&quot;,  # Use MLP if Triton is not available</span>
<span class="sd">                    &quot;dropout&quot;: MLP_DROP,</span>
<span class="sd">                    &quot;activation&quot;: &quot;gelu&quot;,</span>
<span class="sd">                    &quot;hidden_layer_multiplier&quot;: MLP_MULTIPLIER,</span>
<span class="sd">                },</span>
<span class="sd">            }</span>
<span class="sd">        ]</span>


<span class="sd">    .. _`Using the Output Embedding to Improve Language Models`: https://arxiv.org/pdf/1608.05859.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">stack_configs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">xFormerBlockConfig</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">xFormerBlockConfig</span><span class="p">]]</span>
    <span class="n">tie_embedding_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">weight_init</span><span class="p">:</span> <span class="n">xFormerWeightInit</span> <span class="o">=</span> <span class="n">xFormerWeightInit</span><span class="o">.</span><span class="n">ViT</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">stack_configs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]],</span>
        <span class="n">tie_embedding_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">weight_init</span><span class="p">:</span> <span class="n">xFormerWeightInit</span> <span class="o">=</span> <span class="n">xFormerWeightInit</span><span class="o">.</span><span class="n">ViT</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Type all the configurations. Possible typos are caught here</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stack_configs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stack_configs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">stack_configs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;block_type&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;encoder&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">stack_configs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">xFormerEncoderConfig</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">stack_configs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">xFormerDecoderConfig</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stack_configs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">stack_configs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;block_type&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;encoder&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">stack_configs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xFormerEncoderConfig</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">stack_configs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xFormerDecoderConfig</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tie_embedding_weights</span> <span class="o">=</span> <span class="n">tie_embedding_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_init</span> <span class="o">=</span> <span class="n">weight_init</span></div>


<div class="viewcode-block" id="xFormer"><a class="viewcode-back" href="../../../factory/model.html#xformers.factory.model_factory.xFormer">[docs]</a><span class="k">class</span> <span class="nc">xFormer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<div class="viewcode-block" id="xFormer.__init__"><a class="viewcode-back" href="../../../factory/model.html#xformers.factory.model_factory.xFormer.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">stack_configs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">xFormerBlockConfig</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">xFormerBlockConfig</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">xFormerBlockConfig</span><span class="p">]</span>
        <span class="p">],</span>
        <span class="n">tie_embedding_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">weight_init</span><span class="p">:</span> <span class="n">xFormerWeightInit</span> <span class="o">=</span> <span class="n">xFormerWeightInit</span><span class="o">.</span><span class="n">ViT</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a serialized configuration, generate the corresponding model.</span>
<span class="sd">        This is only a helper and can easily be bypassed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stack_configs</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="n">stack_configs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">stack_configs</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

        <span class="c1"># Convenience, users can pass either a list of configs or a single one</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stack_configs</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
            <span class="n">stack_configs</span> <span class="o">=</span> <span class="p">[</span><span class="n">stack_configs</span><span class="p">]</span>

        <span class="c1"># Sanity checks, some config combinations do not make sense</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verify_reversible</span><span class="p">(</span><span class="n">stack_configs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verify_deepnorm</span><span class="p">(</span><span class="n">stack_configs</span><span class="p">)</span>

        <span class="n">encoders</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">decoders</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reversible_encoder</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rev_enc_pose_encoding</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Unroll the configs and build the model</span>
        <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">stack_configs</span><span class="p">:</span>
            <span class="c1"># Handle either Encoder or Decoder stacks</span>
            <span class="n">builder</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">xFormerEncoderBlock</span><span class="o">.</span><span class="n">from_config</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">xFormerEncoderConfig</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">xFormerDecoderBlock</span><span class="o">.</span><span class="n">from_config</span>
            <span class="p">)</span>
            <span class="n">recipient</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">encoders</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">xFormerEncoderConfig</span><span class="p">)</span> <span class="k">else</span> <span class="n">decoders</span>
            <span class="p">)</span>

            <span class="c1"># Build up the stack</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="c1"># Label where this layer is in the stack</span>
                <span class="c1"># (for instance useful for the positional encoding, or late layer norm)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">recipient</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">layer_position</span><span class="o">.</span><span class="n">mark_not_first</span><span class="p">()</span>

                <span class="k">if</span> <span class="n">config</span> <span class="o">!=</span> <span class="n">stack_configs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">config</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">layer_position</span><span class="o">.</span><span class="n">mark_not_last</span><span class="p">()</span>

                <span class="n">block</span> <span class="o">=</span> <span class="n">builder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

                <span class="c1"># If reversible: extract the reversible sub-parts, else append the block as-is</span>
                <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">reversible</span><span class="p">:</span>
                    <span class="c1"># WARNING: only one pose encoding is saved here (not Focal Transformer compatible for instance)</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">xFormerEncoderConfig</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">block</span><span class="o">.</span><span class="n">pose_encoding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">rev_enc_pose_encoding</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">pose_encoding</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">reversible_encoder</span> <span class="o">=</span> <span class="kc">True</span>

                    <span class="n">f</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">xFormerEncoderBlock</span><span class="o">.</span><span class="n">get_reversible_layer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
                    <span class="n">recipient</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">]))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">recipient</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># Tie embedding weights, if requested and possible</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">tie_embedding_weights</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">reversible_encoder</span>
        <span class="p">),</span> <span class="s2">&quot;Reversible layers and  tied embeddings is not supported for now&quot;</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">tie_embedding_weights</span>
            <span class="ow">and</span> <span class="n">encoders</span>
            <span class="ow">and</span> <span class="n">encoders</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">pose_encoding</span>
            <span class="ow">and</span> <span class="n">decoders</span>
            <span class="ow">and</span> <span class="n">decoders</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">pose_encoding</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">reversible</span>
        <span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Tying encoder and decoder embeddings, as requested&quot;</span><span class="p">)</span>
            <span class="n">encoders</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">pose_encoding</span> <span class="o">=</span> <span class="n">decoders</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">pose_encoding</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">rv</span><span class="o">.</span><span class="n">ReversibleSequence</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">encoders</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reversible_encoder</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">encoders</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoders</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">decoders</span><span class="p">)</span>

        <span class="n">use_deepnorm</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">stack_configs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">residual_norm_style</span> <span class="o">==</span> <span class="n">ResidualNormStyle</span><span class="o">.</span><span class="n">DeepNorm</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">use_deepnorm</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">reversible_encoder</span>
        <span class="p">),</span> <span class="s2">&quot;Reversible layers and deepnorm is not supported for now&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">(</span><span class="n">weight_init</span><span class="o">=</span><span class="n">weight_init</span><span class="p">,</span> <span class="n">use_deep_norm</span><span class="o">=</span><span class="n">use_deepnorm</span><span class="p">)</span></div>

<div class="viewcode-block" id="xFormer.from_config"><a class="viewcode-back" href="../../../factory/model.html#xformers.factory.model_factory.xFormer.from_config">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">xFormerConfig</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">stack_configs</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">tie_embedding_weights</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">weight_init</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_verify_reversible</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stack_configs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">xFormerBlockConfig</span><span class="p">]):</span>
        <span class="n">reversible</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">c</span><span class="o">.</span><span class="n">reversible</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="n">stack_configs</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">reversible</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">reversible</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;All layers need to have the same reversibility setting. &quot;</span>
            <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;Currently </span><span class="si">{</span><span class="n">reversible</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_verify_deepnorm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stack_configs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">xFormerBlockConfig</span><span class="p">]):</span>
        <span class="n">deepnorm</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">c</span><span class="o">.</span><span class="n">residual_norm_style</span> <span class="o">==</span> <span class="n">ResidualNormStyle</span><span class="o">.</span><span class="n">DeepNorm</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">stack_configs</span>
        <span class="p">]</span>

        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">deepnorm</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">deepnorm</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;All layers need to have the same deepnorm setting. &quot;</span>
            <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;Currently </span><span class="si">{</span><span class="n">deepnorm</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="xFormer.init_weights"><a class="viewcode-back" href="../../../factory/model.html#xformers.factory.model_factory.xFormer.init_weights">[docs]</a>    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_init</span><span class="p">:</span> <span class="n">xFormerWeightInit</span><span class="p">,</span> <span class="n">use_deep_norm</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="c1"># The deepnorm weight initialization method requires different gain factors for the encoder</span>
        <span class="c1"># and decoder, depending on the general model structure (number of respective layers)</span>
        <span class="k">if</span> <span class="n">use_deep_norm</span><span class="p">:</span>
            <span class="n">encoder_coefficients</span><span class="p">,</span> <span class="n">decoder_coefficients</span> <span class="o">=</span> <span class="n">get_deepnorm_coefficients</span><span class="p">(</span>
                <span class="n">encoder_layers</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">),</span> <span class="n">decoder_layers</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoders</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_coefficients</span><span class="p">,</span> <span class="n">decoder_coefficients</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="n">encoder_gain</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">encoder_coefficients</span><span class="o">.</span><span class="n">beta</span> <span class="k">if</span> <span class="n">encoder_coefficients</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="p">)</span>
        <span class="n">decoder_gain</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decoder_coefficients</span><span class="o">.</span><span class="n">beta</span> <span class="k">if</span> <span class="n">decoder_coefficients</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="p">)</span>

        <span class="c1"># Pick the desired init function</span>
        <span class="n">init_fn</span> <span class="o">=</span> <span class="n">get_weight_init_fn</span><span class="p">(</span><span class="n">weight_init</span><span class="p">)</span>

        <span class="c1"># Initialize all the encoder weights</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="n">init_fn</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">encoder_gain</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoders</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="n">init_fn</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">decoder_gain</span><span class="p">)</span></div>

<div class="viewcode-block" id="xFormer.forward"><a class="viewcode-back" href="../../../factory/model.html#xformers.factory.model_factory.xFormer.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">src</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">tgt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_input_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_input_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>

        <span class="c1"># Encode to latent space if encoder is present</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">encoders</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span>
            <span class="n">memory</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoders</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">encoder</span> <span class="ow">in</span> <span class="n">encoders</span><span class="p">:</span>
                    <span class="n">memory</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">input_mask</span><span class="o">=</span><span class="n">encoder_input_mask</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rev_enc_pose_encoding</span><span class="p">:</span>
                    <span class="n">memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rev_enc_pose_encoding</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>

                <span class="c1"># Reversible Encoder</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">memory</span><span class="p">,</span> <span class="n">memory</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                <span class="c1"># Apply the optional input masking</span>
                <span class="k">if</span> <span class="n">encoder_input_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="n">encoder_input_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">encoder_input_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                    <span class="n">x</span> <span class="o">+=</span> <span class="n">encoder_input_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

                <span class="n">x</span> <span class="o">=</span> <span class="n">encoders</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoders</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">memory</span>

        <span class="c1"># If decoder: either use the encoder ouput, or just decode, both options are possible</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoders</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">tgt</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">if</span> <span class="n">tgt</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">tgt</span>

            <span class="k">for</span> <span class="n">decoder</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoders</span><span class="p">:</span>
                <span class="n">tgt</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                    <span class="n">target</span><span class="o">=</span><span class="n">tgt</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[61]: `memory` is not always initialized here.</span>
                    <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span>
                    <span class="n">input_mask</span><span class="o">=</span><span class="n">decoder_input_mask</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">return</span> <span class="n">tgt</span>

        <span class="k">return</span> <span class="kc">None</span></div></div>
</pre></div>

              </article>
              
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright  2021 Meta Platforms, Inc.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <!-- <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div> -->
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../../../"
    src="../../../_static/documentation_options.js"></script>
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
  <script src="../../../_static/jquery.js"></script>
  <script src="../../../_static/underscore.js"></script>
  <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../../../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/facebookresearch/xformers" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/facebookresearch/xformers">fairscale</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Resources</a></li>
            <li><a href="https://github.com/facebookresearch/xformers">Docs</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://opensource.facebook.com/legal/terms">Terms of Use</a></li>
            <li><a href="https://opensource.facebook.com/legal/privacy">Privacy Policy</a></li>
          </ul>
        </div>
      </div>
    </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/facebookresearch/xformers" aria-label="fairscale"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Docs</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>
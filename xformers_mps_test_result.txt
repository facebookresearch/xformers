âœ“ xFormers imported successfully
âœ“ MPS device available
âœ“ Created tensors: torch.Size([2, 16, 4, 64]) on mps
Testing memory_efficient_attention...
âœ“ memory_efficient_attention successful: torch.Size([2, 16, 4, 64]), device: mps:0
Testing with causal mask...
âœ“ Causal attention successful: torch.Size([2, 16, 4, 64]), device: mps:0
Testing gradient computation...
âš  Gradient computation failed (expected for MPS): element 0 of tensors does not require grad and does not have a grad_fn
ðŸŽ‰ Full xFormers MPS integration working!

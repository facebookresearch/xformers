


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>xformers.ops.fmha.attn_bias | xFormers 0.0.32 documentation</title>
  
  <script src="../../../../_static/js/ga.js"></script>
  <script src="../../../../_static/js/redirect.js"></script>
  
  <link rel="shortcut icon" href="../../../../_static/images/favicon.png" />
  
  
  <link rel="canonical" href="https://facebookresearch.github.io/xformers_modules/xformers/ops/fmha/attn_bias.html" />
  
  <meta property="og:title" content="xformers.ops.fmha.attn_bias | xFormers 0.0.32 documentation">
  <meta name="description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <meta property="og:description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <!--<meta property="og:image" content="https://mmf.sh/img/logo.png">-->
  <!--<meta property="twitter:image" content="https://mmf.sh/img/logo.png">-->
  <meta name="twitter:image:alt" content="Image for xFormers">
  <meta name="twitter:card" content="summary_large_image">
  

  
  
  

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/css/customize.css" type="text/css" />
  <link rel="index" title="Index" href="../../../../genindex.html" />
  <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/facebookresearch/xformers"><img
          src="../../../../_static/logo.png" style="padding-right: 90px;" width="280px" height="53px"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers"> xFormers Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../what_is_xformers.html">What is xFormers?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Components Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../components/index.html">API Reference</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../fmha.html">xformers.ops.fmha</a> &gt;</li>
        
      <li>xformers.ops.fmha.attn_bias</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <h1>Source code for xformers.ops.fmha.attn_bias</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This file contains biases that can be used as the `attn_bias` argument in</span>
<span class="sd">:attr:`xformers.ops.memory_efficient_attention`.</span>
<span class="sd">Essentially, a bias is a Tensor which will be added to the ``Q @ K.t`` before</span>
<span class="sd">computing the ``softmax``.</span>


<span class="sd">The goal of having custom made classes (instead of dense tensors) is that</span>
<span class="sd">we want to avoid having to load the biases from memory in the kernel, for</span>
<span class="sd">performance reasons. We also want to be able to know before-hand which</span>
<span class="sd">parts of the attention matrix we will need to compute (eg causal masks).</span>


<span class="sd">Some very common biases are LowerTriangularMask and BlockDiagonalMask.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">cast</span><span class="p">,</span>
    <span class="n">ClassVar</span><span class="p">,</span>
    <span class="n">Iterable</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_to_device</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">device</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">t</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_to_device_tensor</span><span class="p">(</span><span class="n">seq</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<div class="viewcode-block" id="AttentionBias"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.AttentionBias">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">AttentionBias</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Base class for a custom bias that can be applied \</span>
<span class="sd">        as the attn_bias argument in</span>
<span class="sd">    :attr:`xformers.ops.memory_efficient_attention`.</span>

<span class="sd">    That function has the ability to add a tensor, the</span>
<span class="sd">    attention bias, to the QK^T matrix before it is used</span>
<span class="sd">    in the softmax part of the attention calculation.</span>
<span class="sd">    The attention bias tensor with shape</span>
<span class="sd">    (B or 1, n_queries, number of keys)</span>
<span class="sd">    can be given as the attn_bias input.</span>
<span class="sd">    The most common use case is for an attention bias is</span>
<span class="sd">    to contain only zeros and negative infinities, which forms</span>
<span class="sd">    a mask so that some queries only attend to some keys.</span>

<span class="sd">    Children of this class define alternative things which can</span>
<span class="sd">    be used as the attn_bias input to define an attention bias which</span>
<span class="sd">    forms such a mask, for some common cases.</span>

<span class="sd">    When using an :attr:`xformers.ops.AttentionBias`</span>
<span class="sd">    instead of a :attr:`torch.Tensor`, the mask matrix does</span>
<span class="sd">    not need to be materialized, and can be</span>
<span class="sd">    hardcoded into some kernels for better performance.</span>

<span class="sd">    See:</span>

<span class="sd">    - :attr:`xformers.ops.fmha.attn_bias.LowerTriangularMask`</span>
<span class="sd">    - :attr:`xformers.ops.fmha.attn_bias.LowerTriangularFromBottomRightMask`</span>
<span class="sd">    - :attr:`xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias`</span>
<span class="sd">    - :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalMask`</span>
<span class="sd">    - :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask`</span>

<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="AttentionBias.materialize"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.AttentionBias.materialize">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Materializes the bias as a `torch.Tensor`. This is very slow</span>
<span class="sd">        and we don&#39;t attempt to make it fast. Only use for debugging/testing.</span>

<span class="sd">        Shape should be like `[*, q_seqlen, k_seqlen]`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_default_bias_device</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">mtia</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mtia&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">device</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_materialize_causal_mask</span><span class="p">(</span>
    <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">window_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">from_bottomright</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">create_as</span> <span class="o">=</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
        <span class="n">shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">create_as</span><span class="p">,</span>
        <span class="n">fill_value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">num_queries</span><span class="p">,</span> <span class="n">num_keys</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">from_bottomright</span><span class="p">:</span>
        <span class="n">shift</span> <span class="o">=</span> <span class="n">num_keys</span> <span class="o">-</span> <span class="n">num_queries</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="n">shift</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
    <span class="k">if</span> <span class="n">window_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="n">shift</span> <span class="o">-</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>


<div class="viewcode-block" id="LowerTriangularMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.LowerTriangularMask">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">LowerTriangularMask</span><span class="p">(</span><span class="n">AttentionBias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A lower-triangular (aka causal) mask</span>

<span class="sd">    A query Q cannot attend to a key which is farther from the</span>
<span class="sd">    initial key than Q is from the initial query.</span>

<span class="sd">    See also :attr:`LowerTriangularFromBottomRightMask` if the number</span>
<span class="sd">    of queries is not equal to the number of keys/values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LowerTriangularMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">LowerTriangularMask</span><span class="p">,</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_materialize_causal_mask</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<div class="viewcode-block" id="LowerTriangularMask.add_bias"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.LowerTriangularMask.add_bias">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">add_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LowerTriangularMaskWithTensorBias&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new causal mask with an arbitrary ``torch.Tensor`` bias</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">LowerTriangularMaskWithTensorBias</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LocalAttentionFromBottomRightMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.LocalAttentionFromBottomRightMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LocalAttentionFromBottomRightMask</span><span class="p">(</span><span class="n">AttentionBias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A local attention mask</span>

<span class="sd">    The query at position :math:`q` can attend the key at position :math:`k` if</span>
<span class="sd">    :math:`q - window\\_left &lt;= k + s &lt;= q + window\\_right`</span>

<span class="sd">    With :math:`s = num\\_queries - num\\_keys`</span>

<span class="sd">    :Example:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import torch</span>
<span class="sd">        from xformers.ops import fmha</span>

<span class="sd">        bias = fmha.attn_bias.LocalAttentionFromBottomRightMask(window_left=1, window_right=2)</span>
<span class="sd">        print(bias.materialize(shape=(4, 4)).exp())</span>
<span class="sd">        print(bias.materialize(shape=(4, 5)).exp())</span>

<span class="sd">    .. code-block:: text</span>

<span class="sd">        # 4x4</span>
<span class="sd">        tensor([[1., 1., 1., 0.],</span>
<span class="sd">                [1., 1., 1., 1.],</span>
<span class="sd">                [0., 1., 1., 1.],</span>
<span class="sd">                [0., 0., 1., 1.]])</span>

<span class="sd">        # 4x5</span>
<span class="sd">        tensor([[1., 1., 1., 1., 0.],</span>
<span class="sd">                [0., 1., 1., 1., 1.],</span>
<span class="sd">                [0., 0., 1., 1., 1.],</span>
<span class="sd">                [0., 0., 0., 1., 1.]])</span>

<span class="sd">    :Illustration:</span>

<span class="sd">    .. figure:: /_static/local_attn.png</span>
<span class="sd">        :width: 240px</span>

<span class="sd">        The total window size is :math:`window\\_left + 1 + window\\_right`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">window_left</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">window_right</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LocalAttentionFromBottomRightMask&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_left</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid window value passed to &quot;</span>
                <span class="s2">&quot;`LocalAttentionFromBottomRightMask`: expected&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`window_left &gt; 0` but got window_left=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">window_left</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_right</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid window value passed to &quot;</span>
                <span class="s2">&quot;`LocalAttentionFromBottomRightMask`: expected&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`window_right &gt; 0` but got window_right=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">window_right</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">create_as</span> <span class="o">=</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">create_as</span><span class="p">,</span>
            <span class="n">fill_value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">num_queries</span><span class="p">,</span> <span class="n">num_keys</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">shift</span> <span class="o">=</span> <span class="n">num_keys</span> <span class="o">-</span> <span class="n">num_queries</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="n">shift</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_left</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="n">shift</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_right</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="LowerTriangularFromBottomRightMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.LowerTriangularFromBottomRightMask">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">LowerTriangularFromBottomRightMask</span><span class="p">(</span><span class="n">AttentionBias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A causal masking.</span>

<span class="sd">    This mask is exactly the same as :attr:`LowerTriangularMask` when there is</span>
<span class="sd">    the same number of queries and keys.</span>
<span class="sd">    When the number of queries is different from the number of keys,</span>
<span class="sd">    it is a triangular mask shifted so that the last query can attend to</span>
<span class="sd">    the last key.</span>
<span class="sd">    In other words, a query Q cannot attend to a key which is nearer the</span>
<span class="sd">    final key than Q is to the final query.</span>


<span class="sd">    .. figure:: /_static/causal_bottom_right.png</span>

<span class="sd">        The difference between :attr:`LowerTriangularMask` (left) and</span>
<span class="sd">        :attr:`LowerTriangularFromBottomRightMask` (right). They become</span>
<span class="sd">        equivalent if the number of queries equals the number of keys.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LowerTriangularFromBottomRightMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">LowerTriangularFromBottomRightMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_materialize_causal_mask</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">from_bottomright</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="LowerTriangularFromBottomRightMask.make_local_attention"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.LowerTriangularFromBottomRightMask.make_local_attention">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">make_local_attention</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LowerTriangularFromBottomRightLocalAttentionMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a new bias which combines local + causal attention.</span>

<span class="sd">        See :attr:`LowerTriangularFromBottomRightLocalAttentionMask`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">LowerTriangularFromBottomRightLocalAttentionMask</span><span class="p">(</span><span class="n">window_size</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LowerTriangularFromBottomRightLocalAttentionMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.LowerTriangularFromBottomRightLocalAttentionMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LowerTriangularFromBottomRightLocalAttentionMask</span><span class="p">(</span>
    <span class="n">LowerTriangularFromBottomRightMask</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A mask that combines both :attr:`LowerTriangularFromBottomRightMask` and</span>
<span class="sd">    local attention.</span>

<span class="sd">    A query whose distance from the final query is X cannot attend to a key</span>
<span class="sd">    whose distance to the final key is either of:</span>

<span class="sd">    * less than X (i.e. &quot;causal attention&quot;, same as :attr:`LowerTriangularFromBottomRightMask`)</span>
<span class="sd">    * greater than or equal to X + window_size (i.e. &quot;local attention&quot;)</span>


<span class="sd">    .. figure:: /_static/causal_bottom_right_local.png</span>

<span class="sd">        The mask from :attr:`LowerTriangularFromBottomRightLocalAttentionMask`.</span>
<span class="sd">        The green area is calculated, and the grey area is masked out.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_window_size</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LowerTriangularFromBottomRightLocalAttentionMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">LowerTriangularFromBottomRightLocalAttentionMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected `window_size &gt; 0`, but window_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_materialize_causal_mask</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">window_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="p">,</span>
            <span class="n">from_bottomright</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="LowerTriangularMaskWithTensorBias"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">LowerTriangularMaskWithTensorBias</span><span class="p">(</span><span class="n">LowerTriangularMask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A lower-triangular (aka causal) mask with an additive bias&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span> <span class="o">=</span> <span class="n">bias</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LowerTriangularMaskWithTensorBias&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">LowerTriangularMaskWithTensorBias</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">LowerTriangularMaskWithTensorBias</span><span class="p">(</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">,</span> <span class="n">device</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">materialize</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span></div>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">_SeqLenInfo</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (Internal) Represents the division of a dimension into blocks.</span>

<span class="sd">    For example, to represents a dimension of length 7 divided into</span>
<span class="sd">    three blocks of lengths 2, 3 and 2, use `from_seqlength([2, 3, 2])`.</span>
<span class="sd">    The members will be:</span>
<span class="sd">        max_seqlen: 3</span>
<span class="sd">        min_seqlen: 2</span>
<span class="sd">        seqstart_py: [0, 2, 5, 7]</span>
<span class="sd">        seqstart: torch.IntTensor([0, 2, 5, 7])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">seqstart</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">max_seqlen</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_seqlen</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">seqstart_py</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_SeqLenInfo&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">_SeqLenInfo</span><span class="p">,</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqstart</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">device</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">return</span> <span class="n">_SeqLenInfo</span><span class="p">(</span>
            <span class="n">seqstart</span><span class="o">=</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart</span><span class="p">,</span> <span class="n">device</span><span class="p">),</span>
            <span class="n">max_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seqlen</span><span class="p">,</span>
            <span class="n">min_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_seqlen</span><span class="p">,</span>
            <span class="n">seqstart_py</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">intervals</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
        <span class="k">yield from</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_get_seqstart</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">seqlens</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given sequence lengths, returns the min/max value and the sequence start</span>
<span class="sd">        positions (offsets), with first element being 0 (returned in list and Tensor).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqlens</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="n">seqstart_py</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">max_seqlen</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">min_seqlen</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">for</span> <span class="n">seqlen</span> <span class="ow">in</span> <span class="n">seqlens</span><span class="p">:</span>
            <span class="n">min_seqlen</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">min_seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">)</span> <span class="k">if</span> <span class="n">min_seqlen</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">seqlen</span>
            <span class="n">max_seqlen</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">)</span>
            <span class="n">seqstart_py</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seqstart_py</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">seqstart_py</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">seqlen</span><span class="p">)</span>
        <span class="n">seqstart</span> <span class="o">=</span> <span class="n">_to_device_tensor</span><span class="p">(</span><span class="n">seqstart_py</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">min_seqlen</span><span class="p">,</span> <span class="n">max_seqlen</span><span class="p">,</span> <span class="n">seqstart_py</span><span class="p">,</span> <span class="n">seqstart</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">seqlens</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_SeqLenInfo&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Input tensors are assumed to be in shape [B, M, *]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_default_bias_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">min_seqlen</span><span class="p">,</span> <span class="n">max_seqlen</span><span class="p">,</span> <span class="n">seqstart_py</span><span class="p">,</span> <span class="n">seqstart</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_get_seqstart</span><span class="p">(</span>
            <span class="n">seqlens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">max_seqlen</span><span class="o">=</span><span class="n">max_seqlen</span><span class="p">,</span>
            <span class="n">min_seqlen</span><span class="o">=</span><span class="n">min_seqlen</span><span class="p">,</span>
            <span class="n">seqstart</span><span class="o">=</span><span class="n">seqstart</span><span class="p">,</span>
            <span class="n">seqstart_py</span><span class="o">=</span><span class="n">seqstart_py</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens_inplace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seqlens</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform in-place update. You can only update with the same shape.</span>
<span class="sd">        Can be useful with CUDA graphs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">min_seqlen</span><span class="p">,</span> <span class="n">max_seqlen</span><span class="p">,</span> <span class="n">seqstart_py</span><span class="p">,</span> <span class="n">seqstart</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_seqstart</span><span class="p">(</span>
            <span class="n">seqlens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">seqstart_py</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Old / New len </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">)</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">seqstart</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Contents </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">seqstart</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seqlen</span> <span class="o">&gt;=</span> <span class="n">max_seqlen</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For inplace update, new max_seqlen </span><span class="si">{</span><span class="n">max_seqlen</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;cannot exceed the previous max_seqlen </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seqlen</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seqstart_py</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">seqstart_py</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seqstart</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">seqstart</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">split</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid `torch.Tensor` of shape </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, expected format &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(B, M, *) with B=1 and M=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; seqstart: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">batch_sizes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">split_chunks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch_size</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">:</span>
            <span class="n">split_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="n">it</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="n">it</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">it</span> <span class="o">+=</span> <span class="n">batch_size</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">bs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]])</span>
            <span class="k">for</span> <span class="n">bs</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch_sizes</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_chunks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="p">]</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">_PaddedSeqLenInfo</span><span class="p">(</span><span class="n">_SeqLenInfo</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (Internal)  Represents the division of a dimension into blocks which are</span>
<span class="sd">    padded out to the same total length.</span>

<span class="sd">    For example, to represent a dimension of length 12 with space for</span>
<span class="sd">    three blocks of length 4, but where the occupied lengths are</span>
<span class="sd">    2, 3 and 2, use `from_seqlens_padded([2, 3, 2], 4)`.</span>

<span class="sd">    The layout along the dimension is</span>

<span class="sd">     0 ─►  block 0</span>
<span class="sd">           block 0</span>
<span class="sd">           &lt;space&gt;</span>
<span class="sd">           &lt;space&gt;</span>
<span class="sd">     4 ─►  block 1</span>
<span class="sd">           block 1</span>
<span class="sd">           block 1</span>
<span class="sd">           &lt;space&gt;</span>
<span class="sd">     8 ─►  block 2</span>
<span class="sd">           block 2</span>
<span class="sd">           &lt;space&gt;</span>
<span class="sd">           &lt;space&gt;</span>
<span class="sd">    12 ─►</span>

<span class="sd">    The members will be:</span>
<span class="sd">        max_seqlen: 3</span>
<span class="sd">        min_seqlen: 2</span>
<span class="sd">        seqstart_py: [0, 4, 8, 12]</span>
<span class="sd">        seqstart: torch.IntTensor([0, 4, 8, 12])</span>
<span class="sd">        seqlen_py: [2, 3, 2]</span>
<span class="sd">        seqlen: torch.IntTensor([2, 3, 2])</span>
<span class="sd">        padding: 4</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">seqlen</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">seqlen_py</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span>
    <span class="c1"># From parent: seqstart[i] contains the start position</span>
    <span class="c1"># of the i-th sequence</span>
    <span class="c1"># seqstart: torch.Tensor</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_PaddedSeqLenInfo&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">_PaddedSeqLenInfo</span><span class="p">,</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqlen</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">device</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">return</span> <span class="n">_PaddedSeqLenInfo</span><span class="p">(</span>
            <span class="c1"># _SeqLenInfo</span>
            <span class="n">seqstart</span><span class="o">=</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart</span><span class="p">,</span> <span class="n">device</span><span class="p">),</span>
            <span class="n">max_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seqlen</span><span class="p">,</span>
            <span class="n">min_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_seqlen</span><span class="p">,</span>
            <span class="n">seqstart_py</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">,</span>
            <span class="c1"># _PaddedSeqLenInfo</span>
            <span class="n">seqlen</span><span class="o">=</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">device</span><span class="p">),</span>
            <span class="n">seqlen_py</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">intervals</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">length</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">start</span><span class="p">,</span> <span class="n">start</span> <span class="o">+</span> <span class="n">length</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">seqlens</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_SeqLenInfo&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Use either `_SeqLenInfo.from_seqlens` or `_PaddedSeqLenInfo.from_seqlens_padded`&quot;</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens_padded</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">seqlens</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_PaddedSeqLenInfo&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Input tensors are assumed to be in shape [B, M, *]</span>
<span class="sd">        seqstart = padding * torch.arange(batch_size)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqlens</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">seqlen</span> <span class="o">&lt;=</span> <span class="n">padding</span> <span class="k">for</span> <span class="n">seqlen</span> <span class="ow">in</span> <span class="n">seqlens</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Seqlens </span><span class="si">{</span><span class="n">seqlens</span><span class="si">}</span><span class="s2"> Padding </span><span class="si">{</span><span class="n">padding</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_default_bias_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">seqstart_py</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">seqlens</span><span class="p">)</span> <span class="o">*</span> <span class="n">padding</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="p">))</span>
        <span class="n">seqlen</span> <span class="o">=</span> <span class="n">_to_device_tensor</span><span class="p">(</span><span class="n">seqlens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">seqlen</span><span class="o">=</span><span class="n">seqlen</span><span class="p">,</span>
            <span class="n">seqlen_py</span><span class="o">=</span><span class="n">seqlens</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqlens</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="n">seqlens</span><span class="p">),</span>
            <span class="n">max_seqlen</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">seqlens</span><span class="p">),</span>
            <span class="n">min_seqlen</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="n">seqlens</span><span class="p">),</span>
            <span class="n">seqstart</span><span class="o">=</span><span class="n">_to_device_tensor</span><span class="p">(</span><span class="n">seqstart_py</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
            <span class="n">seqstart_py</span><span class="o">=</span><span class="n">seqstart_py</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens_padded_inplace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seqlens</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform in-place update. You can only update with the same shape.</span>
<span class="sd">        Can be useful with CUDA graphs.</span>
<span class="sd">        Note: we don&#39;t update padding because they would have been already baked</span>
<span class="sd">        into CUDA graphs during the generation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqlens</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">seqlen</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="k">for</span> <span class="n">seqlen</span> <span class="ow">in</span> <span class="n">seqlens</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Seqlens </span><span class="si">{</span><span class="n">seqlens</span><span class="si">}</span><span class="s2"> Padding </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">seqlen_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">seqlens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">seqlens</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Old/New len </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">)</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">seqlens</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Contents </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">seqlens</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seqlen</span> <span class="o">&gt;=</span> <span class="nb">max</span><span class="p">(</span><span class="n">seqlens</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For inplace update, new max_seqlen </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="n">seqlens</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;cannot exceed the previous max_seqlen </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seqlen</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">seqlens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">seqlen</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">seqlen_tensor</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">split</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;_PaddedSeqLenInfo.split&quot;</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">_GappySeqInfo</span><span class="p">(</span><span class="n">_SeqLenInfo</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (Internal) Flexible equivalent of _PaddedSeqLenInfo. There are two</span>
<span class="sd">    distinct semantics.</span>

<span class="sd">    (1) For non-paged masks:</span>
<span class="sd">    Represents the division of a dimension into blocks which are</span>
<span class="sd">    anywhere. Each just has a start and a length. The final start is the total</span>
<span class="sd">    length of the dimension.</span>

<span class="sd">    For example, to represent a dimension of length 14 like follows with</span>
<span class="sd">    three occupied lengths of</span>
<span class="sd">    6, 3 and 1, use `from_seqlens_padded([0, 7, 12, 14], [6, 3, 1])`.</span>

<span class="sd">    The layout along the dimension is</span>

<span class="sd">     0 ─►  block 0</span>
<span class="sd">           block 0</span>
<span class="sd">           block 0</span>
<span class="sd">           block 0</span>
<span class="sd">     4 ─►  block 0</span>
<span class="sd">           block 0</span>
<span class="sd">           &lt;space&gt;</span>
<span class="sd">           block 1</span>
<span class="sd">     8 ─►  block 1</span>
<span class="sd">           block 1</span>
<span class="sd">           &lt;space&gt;</span>
<span class="sd">           &lt;space&gt;</span>
<span class="sd">     12 ─► block 2</span>
<span class="sd">           &lt;space&gt;</span>

<span class="sd">    The members will be:</span>
<span class="sd">        max_seqlen: 6</span>
<span class="sd">        min_seqlen: 1</span>
<span class="sd">        seqstart_py: [0, 7, 12, 14]</span>
<span class="sd">        seqstart: torch.IntTensor([0, 7, 12, 14])</span>
<span class="sd">        seqlen_py: [6, 3, 1]</span>
<span class="sd">        seqlen: torch.IntTensor([6, 3, 1])</span>

<span class="sd">    (2) For paged masks:</span>
<span class="sd">    The notional space is divided into batch-size-many blocks.</span>
<span class="sd">    seqstart and seqstart_py is an offset in the block, not in</span>
<span class="sd">    the whole space, and the extra last element is not important.</span>
<span class="sd">    And seqlen is the index of the last key in the block.</span>
<span class="sd">    Otherwise as above.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">seqlen</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">seqlen_py</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="c1"># From parent: seqstart[i] contains the start position</span>
    <span class="c1"># of the i-th sequence</span>
    <span class="c1"># seqstart: torch.Tensor</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_GappySeqInfo&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">_GappySeqInfo</span><span class="p">,</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqlen</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">device</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">return</span> <span class="n">_GappySeqInfo</span><span class="p">(</span>
            <span class="c1"># _SeqLenInfo</span>
            <span class="n">seqstart</span><span class="o">=</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart</span><span class="p">,</span> <span class="n">device</span><span class="p">),</span>
            <span class="n">max_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seqlen</span><span class="p">,</span>
            <span class="n">min_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_seqlen</span><span class="p">,</span>
            <span class="n">seqstart_py</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">,</span>
            <span class="c1"># _GappySeqInfo</span>
            <span class="n">seqlen</span><span class="o">=</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">device</span><span class="p">),</span>
            <span class="n">seqlen_py</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">intervals</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">length</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">start</span><span class="p">,</span> <span class="n">start</span> <span class="o">+</span> <span class="n">length</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">seqlens</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_SeqLenInfo&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens_gappy</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">seqstarts</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">seqlens</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">paged</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_GappySeqInfo&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seqlens</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="n">seqstart_py</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">seqstarts</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">seqlens</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No elements&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">seqstarts</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">seqlens</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;len(seqstarts)=</span><span class="si">{</span><span class="n">seqstarts</span><span class="si">}</span><span class="s2"> should be len(seqlens)=</span><span class="si">{</span><span class="n">seqlens</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">max_seqlen</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">seqlens</span><span class="p">)</span>
        <span class="n">min_seqlen</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">seqlens</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">paged</span><span class="p">:</span>
            <span class="n">seqstart_py</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">seqlens</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">j</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">seqstart_py</span><span class="p">,</span> <span class="n">seqlens</span><span class="p">)]</span>
        <span class="n">seqlen</span> <span class="o">=</span> <span class="n">_to_device_tensor</span><span class="p">(</span><span class="n">seqlens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">seqlen</span><span class="o">=</span><span class="n">seqlen</span><span class="p">,</span>
            <span class="n">seqlen_py</span><span class="o">=</span><span class="n">seqlens</span><span class="p">,</span>
            <span class="n">max_seqlen</span><span class="o">=</span><span class="n">max_seqlen</span><span class="p">,</span>
            <span class="n">min_seqlen</span><span class="o">=</span><span class="n">min_seqlen</span><span class="p">,</span>
            <span class="n">seqstart</span><span class="o">=</span><span class="n">_to_device_tensor</span><span class="p">(</span><span class="n">seqstart_py</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
            <span class="n">seqstart_py</span><span class="o">=</span><span class="n">seqstart_py</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">split</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;_GappySeqInfo.split&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="BlockDiagonalMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalMask</span><span class="p">(</span><span class="n">AttentionBias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A block-diagonal mask that can be passed as ``attn_bias``</span>
<span class="sd">    argument to :attr:`xformers.ops.memory_efficient_attention`.</span>

<span class="sd">    Queries and Keys are each divided into the same number of blocks.</span>
<span class="sd">    Queries in block i only attend to keys in block i.</span>

<span class="sd">    .. figure:: /_static/block_diag_bias.png</span>

<span class="sd">        This bias can be used to handle a batch of sequences of</span>
<span class="sd">        different lengths, via :attr:`BlockDiagonalMask.from_tensor_list`</span>

<span class="sd">    :Example:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import torch</span>
<span class="sd">        from xformers.ops import fmha</span>

<span class="sd">        K = 16</span>
<span class="sd">        dtype = torch.float16</span>
<span class="sd">        device = &quot;cuda&quot;</span>
<span class="sd">        list_x = [</span>
<span class="sd">            torch.randn([1, 3, 1, K], dtype=dtype, device=device),</span>
<span class="sd">            torch.randn([1, 6, 1, K], dtype=dtype, device=device),</span>
<span class="sd">            torch.randn([1, 2, 1, K], dtype=dtype, device=device),</span>
<span class="sd">        ]</span>
<span class="sd">        attn_bias, x = fmha.BlockDiagonalMask.from_tensor_list(list_x)</span>
<span class="sd">        linear = torch.nn.Linear(K, K * 3).to(device=device, dtype=dtype)</span>

<span class="sd">        q, k, v = linear(x).reshape([1, -1, 1, 3, K]).unbind(-2)</span>
<span class="sd">        out = fmha.memory_efficient_attention(q, k, v, attn_bias=attn_bias)</span>
<span class="sd">        list_out = attn_bias.split(out)</span>
<span class="sd">        print(list_out[0].shape)  # [1, 3, 1, K]</span>
<span class="sd">        assert tuple(list_out[0].shape) == (1, 3, 1, K)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">q_seqinfo</span><span class="p">:</span> <span class="n">_SeqLenInfo</span>
    <span class="n">k_seqinfo</span><span class="p">:</span> <span class="n">_SeqLenInfo</span>
    <span class="n">_batch_sizes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalMask</span><span class="p">,</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">_batch_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_block_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BlockDiagonalMask.materialize"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalMask.materialize">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Materialize the attention bias - for debugging &amp; testing&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
            <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
            <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">mask</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">q_start</span><span class="p">,</span> <span class="n">q_end</span><span class="p">),</span> <span class="p">(</span><span class="n">k_start</span><span class="p">,</span> <span class="n">k_end</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">q_start</span><span class="p">:</span><span class="n">q_end</span><span class="p">,</span> <span class="n">k_start</span><span class="p">:</span><span class="n">k_end</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_block_mask</span><span class="p">(</span>
                <span class="p">(</span><span class="n">q_end</span> <span class="o">-</span> <span class="n">q_start</span><span class="p">,</span> <span class="n">k_end</span> <span class="o">-</span> <span class="n">k_start</span><span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mask</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="BlockDiagonalMask.from_seqlens"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalMask.from_seqlens">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">q_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_seqlen</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a :attr:`BlockDiagonalMask` from a list of tensors lengths for query and key/value.</span>

<span class="sd">        Args:</span>
<span class="sd">            q_seqlen (Union[Sequence[int], torch.Tensor]): List or tensor of sequence lengths for query tensors</span>
<span class="sd">            kv_seqlen (Union[Sequence[int], torch.Tensor], optional): List or tensor of sequence lengths for key/value.</span>
<span class="sd">                    (Defaults to ``q_seqlen``.)</span>
<span class="sd">        Returns:</span>
<span class="sd">            BlockDiagonalMask</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_default_bias_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">kv_seqlen</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">)</span>
        <span class="n">q_seqinfo</span> <span class="o">=</span> <span class="n">_SeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">kv_seqlen</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">q_seqlen</span> <span class="o">==</span> <span class="n">kv_seqlen</span><span class="p">:</span>
            <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">q_seqinfo</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">_SeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">q_seqinfo</span><span class="o">=</span><span class="n">q_seqinfo</span><span class="p">,</span> <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">k_seqinfo</span><span class="p">)</span></div>

<div class="viewcode-block" id="BlockDiagonalMask.from_tensor_list"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalMask.from_tensor_list">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_tensor_list</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">tensors</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="s2">&quot;BlockDiagonalMask&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a :attr:`BlockDiagonalMask` from a list of tensors, and returns the tensors</span>
<span class="sd">        concatenated on the sequence length dimension</span>

<span class="sd">        .. figure:: /_static/block_diag_cat_split.png</span>

<span class="sd">            See also :attr:`BlockDiagonalMask.split` to split the returned</span>
<span class="sd">            :attr:`torch.Tensor` back to a list of tensors of varying sequence length</span>

<span class="sd">        Args:</span>
<span class="sd">            tensors (Sequence[torch.Tensor]): A list of tensors of shape ``[B, M_i, *]``.</span>
<span class="sd">                All tensors should have the same dimension and the same batch size ``B``, but</span>
<span class="sd">                they can have different sequence length ``M``.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[BlockDiagonalMask, torch.Tensor]: The corresponding bias for the attention</span>
<span class="sd">            along with `tensors` concatenated on the sequence length dimension, with shape ``[1, sum_i{M_i}, *]``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>
        <span class="n">seqlens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">seqlens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">block_diag</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">seqlens</span><span class="p">)</span>
        <span class="n">block_diag</span><span class="o">.</span><span class="n">_batch_sizes</span> <span class="o">=</span> <span class="n">batch_sizes</span>
        <span class="n">tensors_bs1</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">)</span>
        <span class="n">concat_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tensors_bs1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">block_diag</span><span class="p">,</span> <span class="n">concat_tensors</span></div>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_tensor_lists_qkv</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">tensors_q</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">tensors_k</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">tensors_v</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="s2">&quot;BlockDiagonalMask&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors_q</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors_k</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">tensors_v</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors_v</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors_q</span><span class="p">)</span>
        <span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors_q</span><span class="p">]</span>
        <span class="n">q_seqlens</span><span class="p">,</span> <span class="n">kv_seqlens</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">tensors_q</span><span class="p">,</span> <span class="n">tensors_k</span><span class="p">)):</span>
            <span class="k">assert</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">q_seqlens</span> <span class="o">+=</span> <span class="p">[</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">*</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">kv_seqlens</span> <span class="o">+=</span> <span class="p">[</span><span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">*</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">tensors_v</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">tensors_v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">block_diag</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">q_seqlens</span><span class="p">,</span> <span class="n">kv_seqlens</span><span class="p">)</span>
        <span class="n">block_diag</span><span class="o">.</span><span class="n">_batch_sizes</span> <span class="o">=</span> <span class="n">batch_sizes</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">block_diag</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_q</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">tensors_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">split_queries</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">split_kv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">)</span>

<div class="viewcode-block" id="BlockDiagonalMask.split"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalMask.split">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The inverse operation of :attr:`BlockDiagonalCausalMask.from_tensor_list`</span>

<span class="sd">        Args:</span>
<span class="sd">            tensor (torch.Tensor): Tensor of tokens of shape ``[1, sum_i{M_i}, *]``</span>

<span class="sd">        Returns:</span>
<span class="sd">            Sequence[torch.Tensor]: A list of tokens with possibly different sequence lengths</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">)</span></div>

<div class="viewcode-block" id="BlockDiagonalMask.make_causal"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalMask.make_causal">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">make_causal</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Makes each block causal&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="p">,</span>
            <span class="n">_batch_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="BlockDiagonalMask.make_causal_from_bottomright"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalMask.make_causal_from_bottomright">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">make_causal_from_bottomright</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalFromBottomRightMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Makes each block causal with a possible non-causal prefix&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalFromBottomRightMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="p">,</span>
            <span class="n">_batch_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="BlockDiagonalMask.make_local_attention"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalMask.make_local_attention">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">make_local_attention</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalLocalAttentionMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Experimental: Makes each block causal with local attention&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalLocalAttentionMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="p">,</span>
            <span class="n">_batch_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">,</span>
            <span class="n">_window_size</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="BlockDiagonalMask.make_local_attention_from_bottomright"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalMask.make_local_attention_from_bottomright">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">make_local_attention_from_bottomright</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalLocalAttentionFromBottomRightMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Experimental: Makes each block causal with local attention, start from bottom right&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalLocalAttentionFromBottomRightMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="p">,</span>
            <span class="n">_batch_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">,</span>
            <span class="n">_window_size</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="BlockDiagonalCausalMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalCausalMask</span><span class="p">(</span><span class="n">BlockDiagonalMask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalMask`, except that each block is causal.</span>

<span class="sd">    Queries and Keys are each divided into the same number of blocks.</span>
<span class="sd">    A query Q in block i cannot attend to a key which is not in block i,</span>
<span class="sd">    nor one which is farther from the initial key in block i than Q</span>
<span class="sd">    is from the initial query in block i.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalCausalMask</span><span class="p">,</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">_batch_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_block_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">LowerTriangularMask</span><span class="p">()</span><span class="o">.</span><span class="n">materialize</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="BlockDiagonalCausalFromBottomRightMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalCausalFromBottomRightMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalCausalFromBottomRightMask</span><span class="p">(</span><span class="n">BlockDiagonalMask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalMask`, except that each block is causal.</span>
<span class="sd">    This mask allows for a non-causal prefix</span>
<span class="sd">    NOTE: Each block should have `num_keys &gt;= num_queries` otherwise the forward pass is not</span>
<span class="sd">    defined (softmax of vector of `-inf` in the attention)</span>

<span class="sd">    Queries and keys are each divided into the same number of blocks.</span>
<span class="sd">    A query Q in block i cannot attend to a key which is not in block i,</span>
<span class="sd">    nor one which nearer the final key in block i than Q is to the</span>
<span class="sd">    final query in block i.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalFromBottomRightMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalCausalFromBottomRightMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalFromBottomRightMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">_batch_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">q_start</span><span class="p">,</span> <span class="n">q_end</span><span class="p">),</span> <span class="p">(</span><span class="n">k_start</span><span class="p">,</span> <span class="n">k_end</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">num_queries</span> <span class="o">=</span> <span class="n">q_end</span> <span class="o">-</span> <span class="n">q_start</span>
            <span class="n">num_keys</span> <span class="o">=</span> <span class="n">k_end</span> <span class="o">-</span> <span class="n">k_start</span>
            <span class="k">if</span> <span class="n">num_keys</span> <span class="o">&lt;</span> <span class="n">num_queries</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Block #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> has num_keys=</span><span class="si">{</span><span class="n">num_keys</span><span class="si">}</span><span class="s2"> and num_queries=</span><span class="si">{</span><span class="n">num_queries</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="s2">&quot; Expected `num_keys &gt;= num_queries`&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_block_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">LowerTriangularFromBottomRightMask</span><span class="p">()</span><span class="o">.</span><span class="n">materialize</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="BlockDiagonalPaddedKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalPaddedKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalPaddedKeysMask</span><span class="p">(</span><span class="n">AttentionBias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalMask`,</span>
<span class="sd">    except we support padding for k/v</span>

<span class="sd">    The keys and values are divided into blocks which are padded out to</span>
<span class="sd">    the same total length.</span>
<span class="sd">    For example, if there is space for 12 keys, for three blocks of</span>
<span class="sd">    max length 4, but we only want to use the first 2, 3 and 2</span>
<span class="sd">    of each block, use `kv_padding=4` and `kv_seqlens=[2, 3, 2]`.</span>
<span class="sd">    The queries are divided into blocks, without padding, of lengths given by</span>
<span class="sd">    q_seqlen.</span>

<span class="sd">    A query Q in block i cannot attend to a key which is not in block i,</span>
<span class="sd">    nor one which is not in use (i.e. in the padded area).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">q_seqinfo</span><span class="p">:</span> <span class="n">_SeqLenInfo</span>
    <span class="n">k_seqinfo</span><span class="p">:</span> <span class="n">_PaddedSeqLenInfo</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalPaddedKeysMask</span><span class="p">,</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalPaddedKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_block_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<div class="viewcode-block" id="BlockDiagonalPaddedKeysMask.materialize"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalPaddedKeysMask.materialize">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Materialize the attention bias - for debugging &amp; testing&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;k shapes wrong&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;q shapes wrong&quot;</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">mask</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">q_start</span><span class="p">,</span> <span class="n">q_end</span><span class="p">),</span> <span class="p">(</span><span class="n">k_start</span><span class="p">,</span> <span class="n">k_end</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">q_start</span><span class="p">:</span><span class="n">q_end</span><span class="p">,</span> <span class="n">k_start</span><span class="p">:</span><span class="n">k_end</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_block_mask</span><span class="p">(</span>
                <span class="p">(</span><span class="n">q_end</span> <span class="o">-</span> <span class="n">q_start</span><span class="p">,</span> <span class="n">k_end</span> <span class="o">-</span> <span class="n">k_start</span><span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mask</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="BlockDiagonalPaddedKeysMask.from_seqlens"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalPaddedKeysMask.from_seqlens">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">q_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_padding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kv_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">causal_diagonal</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalPaddedKeysMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a :attr:`BlockDiagonalPaddedKeysMask` from a list of tensor</span>
<span class="sd">        lengths for query and key/value.</span>

<span class="sd">        Args:</span>
<span class="sd">            q_seqlen (Sequence[int]): List or tensor of sequence lengths for query tensors</span>
<span class="sd">            kv_padding (int): Padding for k/v - also an upperbound on each individual key length</span>
<span class="sd">            kv_seqlen (Sequence[int]): List or tensor of sequence lengths for key/value.</span>
<span class="sd">            causal_diagonal: unused, for BC only</span>
<span class="sd">        Returns:</span>
<span class="sd">            BlockDiagonalPaddedKeysMask</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_default_bias_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">kv_seqlen</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">),</span> <span class="p">(</span>
            <span class="n">q_seqlen</span><span class="p">,</span>
            <span class="n">kv_seqlen</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">q_seqinfo</span> <span class="o">=</span> <span class="n">_SeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">_PaddedSeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens_padded</span><span class="p">(</span>
            <span class="n">kv_seqlen</span><span class="p">,</span> <span class="n">kv_padding</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">q_seqinfo</span><span class="o">=</span><span class="n">q_seqinfo</span><span class="p">,</span> <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">k_seqinfo</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">make_paged</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">block_tables</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">page_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">paged_type</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;PagedBlockDiagonalPaddedKeysMask&quot;</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PagedBlockDiagonalPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="n">paged_bias</span> <span class="o">=</span> <span class="n">paged_type</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">_PaddedSeqLenInfo</span><span class="p">(</span>
                <span class="n">seqstart</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart</span><span class="p">,</span>
                <span class="n">seqstart_py</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">,</span>
                <span class="n">seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqlen</span><span class="p">,</span>
                <span class="n">seqlen_py</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">block_tables</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">page_size</span><span class="p">,</span>
                <span class="n">max_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">max_seqlen</span><span class="p">,</span>
                <span class="n">min_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">min_seqlen</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">block_tables</span><span class="o">=</span><span class="n">block_tables</span><span class="p">,</span>
            <span class="n">page_size</span><span class="o">=</span><span class="n">page_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">paged_bias</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">make_local_attention</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">window_left</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">window_right</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalLocalAttentionPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">BlockDiagonalLocalAttentionPaddedKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="p">,</span>
            <span class="n">window_left</span><span class="o">=</span><span class="n">window_left</span><span class="p">,</span>
            <span class="n">window_right</span><span class="o">=</span><span class="n">window_right</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="BlockDiagonalCausalWithOffsetPaddedKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">(</span><span class="n">BlockDiagonalPaddedKeysMask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask`,</span>
<span class="sd">    except an offset on causality is allowed for each block and we support padding for k/v</span>

<span class="sd">    The keys and values are divided into blocks which are padded out to</span>
<span class="sd">    the same total length.</span>
<span class="sd">    For example, if there is space for 12 keys, for three blocks of</span>
<span class="sd">    max length 4, but we only want to use the first 2, 3 and 2</span>
<span class="sd">    of each block, use `kv_padding=4` and `kv_seqlens=[2, 3, 2]`.</span>
<span class="sd">    The queries are divided into blocks, without padding, of lengths given by</span>
<span class="sd">    q_seqlen.</span>

<span class="sd">    A query Q in block i cannot attend to a key which is not in block i,</span>
<span class="sd">    nor one which is not in use (i.e. in the padded area),</span>
<span class="sd">    nor one which is nearer to the final key in block i</span>
<span class="sd">    than Q is to the final query in block i.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">causal_diagonal</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># unused. Exists for BC only.</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalWithOffsetPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_block_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">LowerTriangularFromBottomRightMask</span><span class="p">()</span><span class="o">.</span><span class="n">materialize</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">q_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_padding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kv_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">causal_diagonal</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalWithOffsetPaddedKeysMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a :attr:`BlockDiagonalCausalWithOffsetPaddedKeysMask` from a list of tensor</span>
<span class="sd">        lengths for query and key/value.</span>

<span class="sd">        Args:</span>
<span class="sd">            q_seqlen (Sequence[int]): List or tensor of sequence lengths for query tensors</span>
<span class="sd">            kv_padding (int): Padding for k/v - also an upperbound on each individual key length</span>
<span class="sd">            kv_seqlen (Sequence[int]): List or tensor of sequence lengths for key/value.</span>
<span class="sd">            causal_diagonal: unused, for BC only</span>
<span class="sd">        Returns:</span>
<span class="sd">            BlockDiagonalCausalWithOffsetPaddedKeysMask</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">kv_seqlen</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">),</span> <span class="p">(</span>
            <span class="n">q_seqlen</span><span class="p">,</span>
            <span class="n">kv_seqlen</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_default_bias_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">q_seqinfo</span> <span class="o">=</span> <span class="n">_SeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">_PaddedSeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens_padded</span><span class="p">(</span>
            <span class="n">kv_seqlen</span><span class="p">,</span> <span class="n">kv_padding</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">q_seqinfo</span><span class="o">=</span><span class="n">q_seqinfo</span><span class="p">,</span> <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">k_seqinfo</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BlockDiagonalLocalAttentionPaddedKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalLocalAttentionPaddedKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalLocalAttentionPaddedKeysMask</span><span class="p">(</span><span class="n">BlockDiagonalPaddedKeysMask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalCausalLocalAttentionPaddedKeysMask`,</span>
<span class="sd">    except that this is non-causal.</span>

<span class="sd">    A query Q in block i cannot attend to a key which is not in block i,</span>
<span class="sd">    nor one which is not in use (i.e. in the padded area),</span>
<span class="sd">    nor one whose distance to the final key in block i</span>
<span class="sd">    is more than window_left further or window_right nearer</span>
<span class="sd">    than Q is to the final query in block i.</span>

<span class="sd">    A query attends to at most window_left + window_right - 1 keys.</span>

<span class="sd">    NOTE that if window_right is 0, then this is like a</span>
<span class="sd">    BlockDiagonalCausalLocalAttentionPaddedKeysMask whose window_size is equal to</span>
<span class="sd">    window_left - 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">window_left</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">window_right</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalLocalAttentionPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalLocalAttentionPaddedKeysMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalLocalAttentionPaddedKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">window_left</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">window_left</span><span class="p">,</span>
            <span class="n">window_right</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">window_right</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_block_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">LocalAttentionFromBottomRightMask</span><span class="p">(</span>
            <span class="n">window_left</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">window_left</span><span class="p">,</span> <span class="n">window_right</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">window_right</span>
        <span class="p">)</span><span class="o">.</span><span class="n">materialize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens_local</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">q_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_padding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kv_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">window_left</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">window_right</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalLocalAttentionPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">kv_seqlen</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">),</span> <span class="p">(</span>
            <span class="n">q_seqlen</span><span class="p">,</span>
            <span class="n">kv_seqlen</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">q_seqinfo</span> <span class="o">=</span> <span class="n">_SeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span>
        <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">_PaddedSeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens_padded</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">,</span> <span class="n">kv_padding</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">k_seqinfo</span><span class="p">,</span>
            <span class="n">window_left</span><span class="o">=</span><span class="n">window_left</span><span class="p">,</span>
            <span class="n">window_right</span><span class="o">=</span><span class="n">window_right</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="BlockDiagonalCausalLocalAttentionPaddedKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalCausalLocalAttentionPaddedKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalCausalLocalAttentionPaddedKeysMask</span><span class="p">(</span><span class="n">BlockDiagonalPaddedKeysMask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask`,</span>
<span class="sd">    except with a window size.</span>

<span class="sd">    A query Q in block i cannot attend to a key which is not in block i,</span>
<span class="sd">    nor one which is not in use (i.e. in the padded area),</span>
<span class="sd">    nor one which is nearer to the final key in block i</span>
<span class="sd">    than Q is to the final query in block i, nor one that is at least</span>
<span class="sd">    window_size further from the final key in block i than Q is</span>
<span class="sd">    to the final query in block i.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_window_size</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalLocalAttentionPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalCausalLocalAttentionPaddedKeysMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalLocalAttentionPaddedKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">_window_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_block_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_materialize_causal_mask</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">window_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="p">,</span>
            <span class="n">from_bottomright</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens_local</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">q_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_padding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kv_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">window_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalLocalAttentionPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">kv_seqlen</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">),</span> <span class="p">(</span>
            <span class="n">q_seqlen</span><span class="p">,</span>
            <span class="n">kv_seqlen</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">q_seqinfo</span> <span class="o">=</span> <span class="n">_SeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span>
        <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">_PaddedSeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens_padded</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">,</span> <span class="n">kv_padding</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">q_seqinfo</span><span class="o">=</span><span class="n">q_seqinfo</span><span class="p">,</span> <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">k_seqinfo</span><span class="p">,</span> <span class="n">_window_size</span><span class="o">=</span><span class="n">window_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="PagedBlockDiagonalPaddedKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.PagedBlockDiagonalPaddedKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PagedBlockDiagonalPaddedKeysMask</span><span class="p">(</span><span class="n">AttentionBias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as BlockDiagonalPaddedKeysMask, but for paged attention.</span>
<span class="sd">    block_tables has shape [batch_size, max_num_pages] and K/V have shape</span>
<span class="sd">    [1, max_num_pages * page_size, num_heads, head_dim]</span>
<span class="sd">    or [1, max_num_pages * page_size, num_groups, num_heads, head_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">q_seqinfo</span><span class="p">:</span> <span class="n">_SeqLenInfo</span>
    <span class="n">k_seqinfo</span><span class="p">:</span> <span class="n">_PaddedSeqLenInfo</span>
    <span class="n">block_tables</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">page_size</span><span class="p">:</span> <span class="nb">int</span>

    <span class="n">_UNPAGED_TYPE</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">BlockDiagonalPaddedKeysMask</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">BlockDiagonalPaddedKeysMask</span>
    <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PagedBlockDiagonalPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">PagedBlockDiagonalPaddedKeysMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">PagedBlockDiagonalPaddedKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">block_tables</span><span class="o">=</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="p">,</span> <span class="n">device</span><span class="p">),</span>
            <span class="n">page_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">page_size</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="PagedBlockDiagonalPaddedKeysMask.materialize"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.PagedBlockDiagonalPaddedKeysMask.materialize">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Materialize the attention bias - for debugging &amp; testing&quot;&quot;&quot;</span>
        <span class="c1"># First create a non-paged mask, then cut individual pages and</span>
        <span class="c1"># copy them to their places in the physical mask, using block tables</span>

        <span class="n">max_row_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
        <span class="n">bias_nonpaged</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_UNPAGED_TYPE</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">_PaddedSeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens_padded</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">,</span> <span class="n">max_row_len</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">mask_nonpaged</span> <span class="o">=</span> <span class="n">bias_nonpaged</span><span class="o">.</span><span class="n">materialize</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="n">n_used_blocks</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">max_physical_len</span> <span class="o">=</span> <span class="n">n_used_blocks</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
        <span class="n">mask_paged</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">mask_nonpaged</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_physical_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">mask_paged</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">q_start</span><span class="p">,</span> <span class="n">q_end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">()):</span>
            <span class="k">for</span> <span class="n">logical_page_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">physical_page_idx</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
                    <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">logical_page_idx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="n">k_logical_start</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">max_row_len</span> <span class="o">+</span> <span class="n">logical_page_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
                <span class="n">k_logical_end</span> <span class="o">=</span> <span class="n">k_logical_start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
                <span class="n">k_physical_start</span> <span class="o">=</span> <span class="n">physical_page_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
                <span class="n">k_physical_end</span> <span class="o">=</span> <span class="n">k_physical_start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
                <span class="n">mask_paged</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">q_start</span><span class="p">:</span><span class="n">q_end</span><span class="p">,</span> <span class="n">k_physical_start</span><span class="p">:</span><span class="n">k_physical_end</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">mask_nonpaged</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">q_start</span><span class="p">:</span><span class="n">q_end</span><span class="p">,</span> <span class="n">k_logical_start</span><span class="p">:</span><span class="n">k_logical_end</span><span class="p">]</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">mask_paged</span></div>

<div class="viewcode-block" id="PagedBlockDiagonalPaddedKeysMask.from_seqlens"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.PagedBlockDiagonalPaddedKeysMask.from_seqlens">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">q_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">block_tables</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">page_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PagedBlockDiagonalPaddedKeysMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a :attr:`PagedBlockDiagonalPaddedKeysMask` from a list of tensor</span>
<span class="sd">        lengths for query and key/value.</span>

<span class="sd">        Args:</span>
<span class="sd">            q_seqlen (Sequence[int]): List or tensor of sequence lengths for query tensors</span>
<span class="sd">            kv_padding (int): Padding for k/v - also an upperbound on each individual key length</span>
<span class="sd">            kv_seqlen (Sequence[int]): List or tensor of sequence lengths for key/value.</span>
<span class="sd">            causal_diagonal: unused, for BC only</span>
<span class="sd">        Returns:</span>
<span class="sd">            PagedBlockDiagonalPaddedKeysMask</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">),</span> <span class="p">(</span>
            <span class="n">q_seqlen</span><span class="p">,</span>
            <span class="n">kv_seqlen</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_default_bias_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">q_seqinfo</span> <span class="o">=</span> <span class="n">_SeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">_PaddedSeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens_padded</span><span class="p">(</span>
            <span class="n">kv_seqlen</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">block_tables</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">page_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">k_seqinfo</span><span class="p">,</span>
            <span class="n">block_tables</span><span class="o">=</span><span class="n">block_tables</span><span class="p">,</span>
            <span class="n">page_size</span><span class="o">=</span><span class="n">page_size</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="PagedBlockDiagonalCausalWithOffsetPaddedKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.PagedBlockDiagonalCausalWithOffsetPaddedKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PagedBlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">(</span>
    <span class="n">PagedBlockDiagonalPaddedKeysMask</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as BlockDiagonalCausalWithOffsetPaddedKeysMask, but for paged attention.</span>
<span class="sd">    block_tables has shape [batch_size, max_num_pages] and K/V have shape</span>
<span class="sd">    [1, max_num_pages * page_size, num_heads, head_dim]</span>
<span class="sd">    or [1, max_num_pages * page_size, num_groups, num_heads, head_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_UNPAGED_TYPE</span> <span class="o">=</span> <span class="n">BlockDiagonalCausalWithOffsetPaddedKeysMask</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PagedBlockDiagonalCausalWithOffsetPaddedKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">PagedBlockDiagonalCausalWithOffsetPaddedKeysMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">PagedBlockDiagonalCausalWithOffsetPaddedKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">block_tables</span><span class="o">=</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="p">,</span> <span class="n">device</span><span class="p">),</span>
            <span class="n">page_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">page_size</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="BlockDiagonalGappyKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalGappyKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalGappyKeysMask</span><span class="p">(</span><span class="n">AttentionBias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalMask`,</span>
<span class="sd">    except k/v is gappy.</span>

<span class="sd">    A query Q in block i only attends to a key which is in block i.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">q_seqinfo</span><span class="p">:</span> <span class="n">_SeqLenInfo</span>
    <span class="n">k_seqinfo</span><span class="p">:</span> <span class="n">_GappySeqInfo</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalGappyKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalGappyKeysMask</span><span class="p">,</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalGappyKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BlockDiagonalGappyKeysMask.materialize"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalGappyKeysMask.materialize">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Materialize the attention bias - for debugging &amp; testing&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;k shapes wrong&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;q shapes wrong&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">))</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">mask</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">q_start</span><span class="p">,</span> <span class="n">q_end</span><span class="p">),</span> <span class="p">(</span><span class="n">k_start</span><span class="p">,</span> <span class="n">k_end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
        <span class="p">):</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">q_start</span><span class="p">:</span><span class="n">q_end</span><span class="p">,</span> <span class="n">k_start</span><span class="p">:</span><span class="n">k_end</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mask</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="BlockDiagonalGappyKeysMask.from_seqlens"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalGappyKeysMask.from_seqlens">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">q_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_seqstarts</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalGappyKeysMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a :attr:`BlockDiagonalGappyKeysMask` from a list of tensor</span>
<span class="sd">        lengths for query and key/value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">),</span> <span class="p">(</span>
            <span class="n">q_seqlen</span><span class="p">,</span>
            <span class="n">kv_seqlen</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_get_default_bias_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">q_seqinfo</span> <span class="o">=</span> <span class="n">_SeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">_GappySeqInfo</span><span class="o">.</span><span class="n">from_seqlens_gappy</span><span class="p">(</span>
            <span class="n">kv_seqstarts</span><span class="p">,</span> <span class="n">kv_seqlen</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">q_seqinfo</span><span class="o">=</span><span class="n">q_seqinfo</span><span class="p">,</span> <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">k_seqinfo</span><span class="p">)</span></div>

<div class="viewcode-block" id="BlockDiagonalGappyKeysMask.make_paged"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalGappyKeysMask.make_paged">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">make_paged</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">block_tables</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">page_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">notional_padding</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">paged_type</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;PagedBlockDiagonalGappyKeysMask&quot;</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AttentionBias</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Assuming our keys actually live in separate blocks of length</span>
<span class="sd">        notional_padding, convert to a Paged version, avoiding GPU syncs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">notional_padding</span> <span class="o">%</span> <span class="n">page_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Notional padding should be divisible by the page size,&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="n">notional_padding</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">page_size</span><span class="si">=}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="n">max_row_len</span> <span class="o">=</span> <span class="n">block_tables</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">page_size</span>
        <span class="n">new_seqstarts_py</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">start</span> <span class="o">-</span> <span class="n">i</span> <span class="o">*</span> <span class="n">notional_padding</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">]</span>
        <span class="n">new_seqstarts_py</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_row_len</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">new_seqstarts_py</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">max_row_len</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">new_seqstarts_py</span><span class="si">=}</span><span class="s2">&quot;</span>

        <span class="c1"># Sequence info is duplicated on CPU and GPU,</span>
        <span class="c1"># but we process them independently to avoid GPU sync.</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">)</span>
        <span class="n">notional_starts</span> <span class="o">=</span> <span class="n">notional_padding</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">block_tables</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">new_seqstarts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart</span> <span class="o">-</span> <span class="n">notional_starts</span>

        <span class="n">new_seqlens_py</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="o">+</span> <span class="n">j</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">new_seqstarts_py</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">new_seqlens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqlen</span> <span class="o">+</span> <span class="n">new_seqstarts</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">_GappySeqInfo</span><span class="p">(</span>
            <span class="n">seqlen</span><span class="o">=</span><span class="n">new_seqlens</span><span class="p">,</span>
            <span class="n">seqlen_py</span><span class="o">=</span><span class="n">new_seqlens_py</span><span class="p">,</span>
            <span class="n">max_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">max_seqlen</span><span class="p">,</span>
            <span class="n">min_seqlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">min_seqlen</span><span class="p">,</span>
            <span class="n">seqstart</span><span class="o">=</span><span class="n">new_seqstarts</span><span class="p">,</span>
            <span class="n">seqstart_py</span><span class="o">=</span><span class="n">new_seqstarts_py</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">max_seqlen</span> <span class="o">&lt;=</span> <span class="n">max_row_len</span>
        <span class="n">paged_bias</span> <span class="o">=</span> <span class="n">paged_type</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">k_seqinfo</span><span class="p">,</span>
            <span class="n">block_tables</span><span class="o">=</span><span class="n">block_tables</span><span class="p">,</span>
            <span class="n">page_size</span><span class="o">=</span><span class="n">page_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">paged_bias</span></div></div>


<div class="viewcode-block" id="BlockDiagonalCausalWithOffsetGappyKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetGappyKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalCausalWithOffsetGappyKeysMask</span><span class="p">(</span><span class="n">BlockDiagonalGappyKeysMask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask`,</span>
<span class="sd">    except k/v is gappy.</span>

<span class="sd">    A query Q in block i cannot attend to a key which is not in block i,</span>
<span class="sd">    nor one which is nearer to the final key in block i</span>
<span class="sd">    than Q is to the final query in block i.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalWithOffsetGappyKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalCausalWithOffsetGappyKeysMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalWithOffsetGappyKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BlockDiagonalCausalWithOffsetGappyKeysMask.materialize"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetGappyKeysMask.materialize">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Materialize the attention bias - for debugging &amp; testing&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;k shapes wrong&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;q shapes wrong&quot;</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">mask</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">q_start</span><span class="p">,</span> <span class="n">q_end</span><span class="p">),</span> <span class="p">(</span><span class="n">k_start</span><span class="p">,</span> <span class="n">k_end</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">(),</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">mask</span><span class="p">[</span>
                <span class="n">q_start</span><span class="p">:</span><span class="n">q_end</span><span class="p">,</span> <span class="n">k_start</span><span class="p">:</span><span class="n">k_end</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">LowerTriangularFromBottomRightMask</span><span class="p">()</span><span class="o">.</span><span class="n">materialize</span><span class="p">(</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">q_end</span> <span class="o">-</span> <span class="n">q_start</span><span class="p">,</span> <span class="n">k_end</span> <span class="o">-</span> <span class="n">k_start</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mask</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="PagedBlockDiagonalGappyKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.PagedBlockDiagonalGappyKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PagedBlockDiagonalGappyKeysMask</span><span class="p">(</span><span class="n">AttentionBias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Equivalent BlockDiagonalGappyKeysMask, but for paged attention.</span>
<span class="sd">    block_tables has shape [batch_size, max_num_pages] and K/V have shape</span>
<span class="sd">    [1, max_num_pages * page_size, num_heads, head_dim]</span>
<span class="sd">    or [1, max_num_pages * page_size, num_groups, num_heads, head_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">q_seqinfo</span><span class="p">:</span> <span class="n">_SeqLenInfo</span>
    <span class="n">k_seqinfo</span><span class="p">:</span> <span class="n">_GappySeqInfo</span>
    <span class="n">block_tables</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">page_size</span><span class="p">:</span> <span class="nb">int</span>

    <span class="n">_UNPAGED_TYPE</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">BlockDiagonalGappyKeysMask</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">BlockDiagonalGappyKeysMask</span>
    <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PagedBlockDiagonalGappyKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">PagedBlockDiagonalGappyKeysMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">PagedBlockDiagonalGappyKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">block_tables</span><span class="o">=</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="p">,</span> <span class="n">device</span><span class="p">),</span>
            <span class="n">page_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">page_size</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="PagedBlockDiagonalGappyKeysMask.materialize"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.PagedBlockDiagonalGappyKeysMask.materialize">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">materialize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Materialize the attention bias - for debugging &amp; testing&quot;&quot;&quot;</span>
        <span class="c1"># First create a non-paged mask, then cut individual pages and</span>
        <span class="c1"># copy them to their places in the physical mask, using block tables</span>

        <span class="n">max_row_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
        <span class="n">new_seqstarts</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">start</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">max_row_len</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">new_seqlens</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
            <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqlen_py</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">bias_nonpaged</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_UNPAGED_TYPE</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">_GappySeqInfo</span><span class="o">.</span><span class="n">from_seqlens_gappy</span><span class="p">(</span>
                <span class="n">new_seqstarts</span><span class="p">,</span>
                <span class="n">new_seqlens</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">mask_nonpaged</span> <span class="o">=</span> <span class="n">bias_nonpaged</span><span class="o">.</span><span class="n">materialize</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="n">n_used_blocks</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">max_physical_len</span> <span class="o">=</span> <span class="n">n_used_blocks</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
        <span class="n">mask_paged</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">mask_nonpaged</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_physical_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">mask_paged</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">q_start</span><span class="p">,</span> <span class="n">q_end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">intervals</span><span class="p">()):</span>
            <span class="k">for</span> <span class="n">logical_page_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">physical_page_idx</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
                    <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">logical_page_idx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="n">k_logical_start</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">max_row_len</span> <span class="o">+</span> <span class="n">logical_page_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
                <span class="n">k_logical_end</span> <span class="o">=</span> <span class="n">k_logical_start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
                <span class="n">k_physical_start</span> <span class="o">=</span> <span class="n">physical_page_idx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
                <span class="n">k_physical_end</span> <span class="o">=</span> <span class="n">k_physical_start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span>
                <span class="n">mask_paged</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">q_start</span><span class="p">:</span><span class="n">q_end</span><span class="p">,</span> <span class="n">k_physical_start</span><span class="p">:</span><span class="n">k_physical_end</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">mask_nonpaged</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">q_start</span><span class="p">:</span><span class="n">q_end</span><span class="p">,</span> <span class="n">k_logical_start</span><span class="p">:</span><span class="n">k_logical_end</span><span class="p">]</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">mask_paged</span></div>

<div class="viewcode-block" id="PagedBlockDiagonalGappyKeysMask.from_seqlens"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.PagedBlockDiagonalGappyKeysMask.from_seqlens">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_seqlens</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">q_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_seqstarts</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">kv_seqlen</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">block_tables</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">page_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PagedBlockDiagonalGappyKeysMask&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a :attr:`PagedBlockDiagonalGappyKeysMask` from a list of tensor</span>
<span class="sd">        lengths for query and key/value.</span>

<span class="sd">        Note that unlike :attr:`BlockDiagonalGappyKeysMask`, kv_seqstarts is</span>
<span class="sd">        addressing in a different space for each batch element. For example</span>
<span class="sd">        if you were doing a BlockDiagonalPaddedKeysMask with two batch</span>
<span class="sd">        elements and padding=100, but wanted to change it so that the first</span>
<span class="sd">        key is ignored, then you would use BlockDiagonalGappyKeysMask with kv_seqstarts</span>
<span class="sd">        [1, 101, 200]. But if you were using PagedBlockDiagonalPaddedKeysMask</span>
<span class="sd">        but wanted to ignore the first key, you would provide this function with</span>
<span class="sd">        kv_seqstarts = [1, 1].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_seqlen</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kv_seqstarts</span><span class="p">),</span> <span class="p">(</span>
            <span class="n">q_seqlen</span><span class="p">,</span>
            <span class="n">kv_seqlen</span><span class="p">,</span>
            <span class="n">kv_seqstarts</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">block_tables</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">device</span>
        <span class="n">q_seqinfo</span> <span class="o">=</span> <span class="n">_SeqLenInfo</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">k_seqinfo</span> <span class="o">=</span> <span class="n">_GappySeqInfo</span><span class="o">.</span><span class="n">from_seqlens_gappy</span><span class="p">(</span>
            <span class="n">kv_seqstarts</span><span class="p">,</span> <span class="n">kv_seqlen</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="n">q_seqinfo</span><span class="p">,</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="n">k_seqinfo</span><span class="p">,</span>
            <span class="n">block_tables</span><span class="o">=</span><span class="n">block_tables</span><span class="p">,</span>
            <span class="n">page_size</span><span class="o">=</span><span class="n">page_size</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="PagedBlockDiagonalCausalWithOffsetGappyKeysMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.PagedBlockDiagonalCausalWithOffsetGappyKeysMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PagedBlockDiagonalCausalWithOffsetGappyKeysMask</span><span class="p">(</span><span class="n">PagedBlockDiagonalGappyKeysMask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as BlockDiagonalCausalWithOffsetGappyKeysMask, but for paged attention.</span>
<span class="sd">    block_tables has shape [batch_size, max_num_pages] and K/V have shape</span>
<span class="sd">    [1, max_num_pages * page_size, num_heads, head_dim] or</span>
<span class="sd">    [1, max_num_pages * page_size, num_groups, num_heads, head_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_UNPAGED_TYPE</span> <span class="o">=</span> <span class="n">BlockDiagonalCausalWithOffsetGappyKeysMask</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PagedBlockDiagonalCausalWithOffsetGappyKeysMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">PagedBlockDiagonalCausalWithOffsetGappyKeysMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">PagedBlockDiagonalCausalWithOffsetGappyKeysMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">block_tables</span><span class="o">=</span><span class="n">_to_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_tables</span><span class="p">,</span> <span class="n">device</span><span class="p">),</span>
            <span class="n">page_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">page_size</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="BlockDiagonalCausalLocalAttentionMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalCausalLocalAttentionMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalCausalLocalAttentionMask</span><span class="p">(</span><span class="n">BlockDiagonalCausalMask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (Experimental feature)</span>
<span class="sd">    Same as :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask`.</span>
<span class="sd">    This makes the mask &quot;local&quot; and the attention pattern banded.</span>

<span class="sd">    The ith query in a block only attends to keys in its block with index</span>
<span class="sd">    greater than i - window_size and less than or equal to i.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_window_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># forced due to inheritance and default arguments</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalLocalAttentionMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalCausalLocalAttentionMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalLocalAttentionMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">_batch_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">,</span>
            <span class="n">_window_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected `window_size &gt; 0`, but window_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">q_seqlen</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">y</span> <span class="o">-</span> <span class="n">x</span>
            <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="p">)</span>
        <span class="p">]</span>
        <span class="n">kv_seqlen</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">y</span> <span class="o">-</span> <span class="n">x</span>
            <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">seqstart_py</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="p">)</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">q_seqlen</span><span class="p">,</span> <span class="n">kv_seqlen</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">q</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span> <span class="o">&gt;=</span> <span class="n">k</span><span class="p">:</span>
                <span class="c1"># Each query only attends to keys no further than window_size back.</span>
                <span class="c1"># When q &gt; k + window_size, there will be a query for which the window doesn&#39;t reach any key.</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;No keys are attended in q_seqlen </span><span class="si">{</span><span class="n">q</span><span class="si">}</span><span class="s2"> k_seqlen </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2"> with sliding window </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_block_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_materialize_causal_mask</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">window_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="BlockDiagonalCausalLocalAttentionFromBottomRightMask"><a class="viewcode-back" href="../../../../components/ops.html#xformers.ops.fmha.attn_bias.BlockDiagonalCausalLocalAttentionFromBottomRightMask">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BlockDiagonalCausalLocalAttentionFromBottomRightMask</span><span class="p">(</span>
    <span class="n">BlockDiagonalCausalFromBottomRightMask</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (Experimental feature)</span>
<span class="sd">    Same as :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask`.</span>
<span class="sd">    This makes the mask &quot;local&quot; and the attention pattern banded.</span>

<span class="sd">    A query with distance j from the last query in its block only attends to</span>
<span class="sd">    keys in the same block, and only those whose distance to the last key</span>
<span class="sd">    in the block is greater than or equal to j and less than window_size + j.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_window_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># forced due to inheritance and default arguments</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BlockDiagonalCausalLocalAttentionFromBottomRightMask&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">BlockDiagonalCausalLocalAttentionFromBottomRightMask</span>
        <span class="p">),</span> <span class="s2">&quot;Please implement in subclass&quot;</span>
        <span class="k">return</span> <span class="n">BlockDiagonalCausalLocalAttentionFromBottomRightMask</span><span class="p">(</span>
            <span class="n">q_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">k_seqinfo</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_seqinfo</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">_batch_sizes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_sizes</span><span class="p">,</span>
            <span class="n">_window_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected `window_size &gt; 0`, but window_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_block_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_materialize_causal_mask</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">window_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_window_size</span><span class="p">,</span>
            <span class="n">from_bottomright</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span></div>


<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">allow_in_graph</span><span class="p">(</span><span class="n">LowerTriangularMask</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">allow_in_graph</span><span class="p">(</span><span class="n">LowerTriangularMaskWithTensorBias</span><span class="p">)</span>

<span class="n">VARLEN_BIASES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">BlockDiagonalMask</span><span class="p">,</span>
    <span class="n">BlockDiagonalGappyKeysMask</span><span class="p">,</span>
    <span class="n">BlockDiagonalPaddedKeysMask</span><span class="p">,</span>
    <span class="n">PagedBlockDiagonalPaddedKeysMask</span><span class="p">,</span>
    <span class="n">PagedBlockDiagonalGappyKeysMask</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

              </article>
              
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright © 2021 Meta Platforms, Inc.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <!-- <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div> -->
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../../../../"
    src="../../../../_static/documentation_options.js"></script>
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
  <script src="../../../../_static/jquery.js"></script>
  <script src="../../../../_static/underscore.js"></script>
  <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../../../../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/facebookresearch/xformers" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/facebookresearch/xformers">fairscale</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Resources</a></li>
            <li><a href="https://github.com/facebookresearch/xformers">Docs</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://opensource.facebook.com/legal/terms">Terms of Use</a></li>
            <li><a href="https://opensource.facebook.com/legal/privacy">Privacy Policy</a></li>
          </ul>
        </div>
      </div>
    </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/facebookresearch/xformers" aria-label="fairscale"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Docs</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>xformers.components.attention.nystrom | xFormers 0.0.29 documentation</title>
  
  <script src="../../../../_static/js/ga.js"></script>
  <script src="../../../../_static/js/redirect.js"></script>
  
  <link rel="shortcut icon" href="../../../../_static/images/favicon.png" />
  
  
  <link rel="canonical" href="https://facebookresearch.github.io/xformers_modules/xformers/components/attention/nystrom.html" />
  
  <meta property="og:title" content="xformers.components.attention.nystrom | xFormers 0.0.29 documentation">
  <meta name="description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <meta property="og:description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <!--<meta property="og:image" content="https://mmf.sh/img/logo.png">-->
  <!--<meta property="twitter:image" content="https://mmf.sh/img/logo.png">-->
  <meta name="twitter:image:alt" content="Image for xFormers">
  <meta name="twitter:card" content="summary_large_image">
  

  
  
  

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/css/customize.css" type="text/css" />
  <link rel="index" title="Index" href="../../../../genindex.html" />
  <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/facebookresearch/xformers"><img
          src="../../../../_static/logo.png" style="padding-right: 90px;" width="280px" height="53px"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers"> xFormers Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../what_is_xformers.html">What is xFormers?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Components Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../components/index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials and examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/index.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Some custom parts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../custom_parts/index.html">Custom parts reference</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../attention.html">xformers.components.attention</a> &gt;</li>
        
      <li>xformers.components.attention.nystrom</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <h1>Source code for xformers.components.attention.nystrom</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>


<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="kn">from</span> <span class="nn">xformers.components.attention</span> <span class="kn">import</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">AttentionConfig</span><span class="p">,</span> <span class="n">register_attention</span>
<span class="kn">from</span> <span class="nn">xformers.components.attention.core</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">scaled_dot_product_attention</span><span class="p">,</span>
    <span class="n">scaled_query_key_softmax</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">xformers.components.attention.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">bool_mask_to_additive</span><span class="p">,</span>
    <span class="n">iterative_pinv</span><span class="p">,</span>
    <span class="n">reshape_key_padding_mask</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;xformers&quot;</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">NystromSelfAttentionConfig</span><span class="p">(</span><span class="n">AttentionConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    num_heads               Number of heads.</span>
<span class="sd">    num_landmarks           Number of landmarks to use for softmax approximation. 64 often sufficient for a good</span>
<span class="sd">                            approximation according to https://arxiv.org/pdf/2102.03902.pdf.</span>
<span class="sd">    causal                  Apply a causal mask, in that the attention cannot be applied to the future.</span>
<span class="sd">    use_razavi_pinverse     If true, use iterative method from (Razavi et al. 2014) to approximate the Moore-Penrose</span>
<span class="sd">                            inverse, otherwise use standard torch inverse.</span>
<span class="sd">    pinverse_original_init  True if using original initialization when calculating Moore-Penrose pseudo inverse using</span>
<span class="sd">                            method from (Razavi et al. 2014).</span>
<span class="sd">                            False if using exact coefficient computation (leads to faster convergence).</span>
<span class="sd">    inv_iterations          Number of iterations for calculating the Moore-Penrose pseudo inverse.</span>
<span class="sd">    v_skip_connection       A module that will take V as input and will be added as a skip connection to the</span>
<span class="sd">                            softmax approximation. A skip connection is added in the paper to help with training.</span>
<span class="sd">    conv_kernel_size        Kernel size for convolution optionally added to help in training.</span>
<span class="sd">                            If v_skip_connection is not specified, this will be used to define the default</span>
<span class="sd">                            depth wise convolution used as a skip connection.</span>
<span class="sd">                            If both conv_kernel_size and v_skip_connection are None, no skip connection will</span>
<span class="sd">                            be added.</span>
<span class="sd">    landmark_pooling        Which module to use when computing landmarks. Default is AdaptiveAvgPool2d.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">landmark_pooling</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span>
    <span class="n">causal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
    <span class="n">pinverse_original_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
    <span class="n">inv_iterations</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">v_skip_connection</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span>
    <span class="n">conv_kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">use_razavi_pinverse</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">AvgPool</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># Average independently for every segment in the sequence dimension</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">segments</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
        <span class="k">assert</span> <span class="n">segments</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;num_landmarks should be smaller than the sequence length&quot;</span>

        <span class="c1"># Dimensions are a match</span>
        <span class="k">if</span> <span class="n">seq_len</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
                <span class="n">segments</span><span class="p">,</span>
                <span class="n">head_dim</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Handle the last segment boundary being off</span>
        <span class="n">n_round</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>

        <span class="n">x_avg_round</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">n_round</span> <span class="o">*</span> <span class="n">segments</span><span class="p">,</span> <span class="p">:]</span>
            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_round</span><span class="p">,</span> <span class="n">segments</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
            <span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">x_avg_off</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">x</span><span class="p">[:,</span> <span class="n">n_round</span> <span class="o">*</span> <span class="n">segments</span> <span class="p">:,</span> <span class="p">:]</span>
            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="n">n_round</span><span class="p">,</span> <span class="n">segments</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
            <span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x_avg_round</span><span class="p">,</span> <span class="n">x_avg_off</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>


<div class="viewcode-block" id="NystromAttention"><a class="viewcode-back" href="../../../../components/attentions.html#xformers.components.attention.NystromAttention">[docs]</a><span class="nd">@register_attention</span><span class="p">(</span><span class="s2">&quot;nystrom&quot;</span><span class="p">,</span> <span class="n">NystromSelfAttentionConfig</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">NystromAttention</span><span class="p">(</span><span class="n">Attention</span><span class="p">):</span>
    <span class="c1"># TODO: update defaults for use_razavi_pinverse and inv_iterations</span>
<div class="viewcode-block" id="NystromAttention.__init__"><a class="viewcode-back" href="../../../../components/attentions.html#xformers.components.attention.NystromAttention.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">landmark_pooling</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_razavi_pinverse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">pinverse_original_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">inv_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>  <span class="c1"># recommended default in paper was 6.</span>
        <span class="n">v_skip_connection</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">conv_kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Nystrom attention mechanism, from Nystromformer_.</span>
<span class="sd">        ::</span>

<span class="sd">            &quot;A Nystrom-based Algorithm for Approximating Self-Attention.&quot;</span>
<span class="sd">            Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V. (2021)</span>

<span class="sd">            Reference codebase: https://github.com/mlpen/Nystromformer</span>

<span class="sd">        .. _Nystromformer: https://arxiv.org/pdf/2102.03902.pdf</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># merged key padding mask and attention mask is not accepted</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_separate_masks</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span> <span class="o">=</span> <span class="n">num_landmarks</span>
        <span class="c1"># TODO: should be able to not have to pass in num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_razavi_pinverse</span> <span class="o">=</span> <span class="n">use_razavi_pinverse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pinverse_original_init</span> <span class="o">=</span> <span class="n">pinverse_original_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inv_iterations</span> <span class="o">=</span> <span class="n">inv_iterations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skip_connection</span> <span class="o">=</span> <span class="n">v_skip_connection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_connection</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">conv_kernel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">skip_connection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">conv_kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">conv_kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">landmark_pooling</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">landmark_pooling</span> <span class="o">=</span> <span class="n">landmark_pooling</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">landmark_pooling</span> <span class="o">=</span> <span class="n">AvgPool</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">)</span>

        <span class="c1"># Optional lower triangular masks for causal attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">causal_mask_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">causal_mask_2</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">causal_mask_3</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># This attention does not support attention masks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">supports_attention_mask</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">supports_key_padding_mask</span> <span class="o">=</span> <span class="kc">True</span></div>

<div class="viewcode-block" id="NystromAttention.forward"><a class="viewcode-back" href="../../../../components/attentions.html#xformers.components.attention.NystromAttention.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        key_padding_mask    Only a key padding mask is accepted here. The size must be (batch size, sequence length) or</span>
<span class="sd">                            (batch size * num_heads, 1, sequence length). If dimensions are not correct, the mask will</span>
<span class="sd">                            be ignored. An additive mask is expected, meaning float values using &quot;-inf&quot; to mask values</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">batched_dim</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">tt</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">q</span><span class="o">.</span><span class="n">device</span><span class="p">}</span>

        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Bool mask found, but an additive mask is expected. Converting but this is slow&quot;</span>
                <span class="p">)</span>

                <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">bool_mask_to_additive</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">reshape_key_padding_mask</span><span class="p">(</span>
                    <span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">batched_dim</span>
                <span class="p">)</span>

            <span class="n">zeros</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">)</span>
            <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">)</span>
            <span class="n">is_masked</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="o">-</span><span class="n">key_padding_mask</span><span class="p">)</span>

            <span class="c1"># _mask takes 1 if the token is not padded, otherwise 0.</span>
            <span class="n">_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_masked</span><span class="p">,</span> <span class="n">zeros</span><span class="p">,</span> <span class="n">ones</span><span class="p">)</span>
            <span class="n">_mask</span> <span class="o">=</span> <span class="n">_mask</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batched_dim</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Mask q and k before pooling</span>
            <span class="c1"># https://github.com/mlpen/Nystromformer/blob/main/code/attention_nystrom.py#L31</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">_mask</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">_mask</span>

            <span class="k">assert</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">batched_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;key_padding_mask has invalid dimensions </span><span class="si">{</span><span class="n">key_padding_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; Must have dimensions </span><span class="si">{</span><span class="n">batched_dim</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="si">}</span><span class="s2"> or (batch_size, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span> <span class="o">&gt;=</span> <span class="n">seq_len</span><span class="p">:</span>
            <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triu_mask</span><span class="p">(</span><span class="n">batched_dim</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">**</span><span class="n">tt</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">key_padding_mask</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mask</span> <span class="o">+</span> <span class="n">key_padding_mask</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="n">att_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">q_landmarks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">landmark_pooling</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            <span class="n">k_landmarks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">landmark_pooling</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">causal_mask_1</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">or</span> <span class="p">(</span><span class="n">batched_dim</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">)</span>
                <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal_mask_1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">causal_mask_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triu_mask</span><span class="p">(</span>
                    <span class="n">batched_dim</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span> <span class="o">**</span><span class="n">tt</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">causal_mask_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triu_mask</span><span class="p">(</span>
                    <span class="n">batched_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span> <span class="o">**</span><span class="n">tt</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">causal_mask_3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triu_mask</span><span class="p">(</span>
                    <span class="n">batched_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">**</span><span class="n">tt</span>
                <span class="p">)</span>

            <span class="n">mask_3</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal_mask_3</span>
            <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">mask_3</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">key_padding_mask</span> <span class="k">if</span> <span class="n">mask_3</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mask_3</span> <span class="o">+</span> <span class="n">key_padding_mask</span>
                <span class="p">)</span>

            <span class="n">kernel_1</span> <span class="o">=</span> <span class="n">scaled_query_key_softmax</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k_landmarks</span><span class="p">,</span> <span class="n">att_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="n">kernel_2</span> <span class="o">=</span> <span class="n">scaled_query_key_softmax</span><span class="p">(</span>
                <span class="n">q</span><span class="o">=</span><span class="n">q_landmarks</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k_landmarks</span><span class="p">,</span> <span class="n">att_mask</span><span class="o">=</span><span class="kc">None</span>
            <span class="p">)</span>
            <span class="n">kernel_3</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
                <span class="n">q</span><span class="o">=</span><span class="n">q_landmarks</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="n">att_mask</span><span class="o">=</span><span class="n">mask_3</span>
            <span class="p">)</span>

            <span class="n">kernel_2_inv</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">iterative_pinv</span><span class="p">(</span>
                    <span class="n">kernel_2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_iterations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pinverse_original_init</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_razavi_pinverse</span>
                <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">kernel_2</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
                    <span class="n">kernel_1</span><span class="p">,</span>
                    <span class="n">kernel_2_inv</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="n">kernel_3</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_connection</span><span class="p">:</span>
            <span class="c1"># Assumption here is that v is 3D.</span>
            <span class="n">v_conv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_connection</span><span class="p">(</span>
                <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">v</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">+=</span> <span class="n">v_conv</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">v_conv</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">v_conv</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>

    <span class="k">def</span> <span class="nf">_triu_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_1</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_2</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_3</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim_2</span><span class="p">,</span> <span class="n">dim_3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span>
            <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
            <span class="n">dim_1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># micro optim, save memory on the batch dimension</span></div>
</pre></div>

              </article>
              
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright  2021 Meta Platforms, Inc.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <!-- <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div> -->
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../../../../"
    src="../../../../_static/documentation_options.js"></script>
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
  <script src="../../../../_static/jquery.js"></script>
  <script src="../../../../_static/underscore.js"></script>
  <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../../../../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/facebookresearch/xformers" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/facebookresearch/xformers">fairscale</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Resources</a></li>
            <li><a href="https://github.com/facebookresearch/xformers">Docs</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://opensource.facebook.com/legal/terms">Terms of Use</a></li>
            <li><a href="https://opensource.facebook.com/legal/privacy">Privacy Policy</a></li>
          </ul>
        </div>
      </div>
    </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/facebookresearch/xformers" aria-label="fairscale"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Docs</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>
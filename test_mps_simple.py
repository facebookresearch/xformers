#!/usr/bin/env python3
"""
Test script for MPS support in xFormers
"""

import torch
import sys
import traceback

def test_mps_support():
    print("=== Testing MPS Support in xFormers ===")

    # Check PyTorch MPS availability
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("‚úì PyTorch MPS backend is available")
        device = torch.device("mps")
    else:
        print("‚úó PyTorch MPS backend not available, using CPU")
        device = torch.device("cpu")

    try:
        # Import xFormers
        import xformers.ops as xops
        print("‚úì xFormers imported successfully")

        # Create test tensors on the appropriate device
        B, M, H, K = 2, 32, 8, 64
        query = torch.randn(B, M, H, K, device=device, dtype=torch.float32)
        key = torch.randn(B, M, H, K, device=device, dtype=torch.float32)
        value = torch.randn(B, M, H, K, device=device, dtype=torch.float32)

        print(f"‚úì Created tensors on {device}: {query.shape}")

        # Test xFormers memory efficient attention
        print("Testing xFormers memory_efficient_attention...")
        with torch.no_grad():
            output = xops.memory_efficient_attention(query, key, value)
        print(f"‚úì xFormers attention successful: {output.shape}, device: {output.device}")

        # Test with causal mask
        print("Testing with causal attention...")
        with torch.no_grad():
            causal_output = xops.memory_efficient_attention(
                query, key, value,
                attn_bias=xops.fmha.attn_bias.LowerTriangularMask()
            )
        print(f"‚úì Causal attention successful: {causal_output.shape}, device: {causal_output.device}")

        # Test gradient computation
        print("Testing gradient computation...")
        query_grad = torch.randn_like(query)
        key_grad = torch.randn_like(key)
        value_grad = torch.randn_like(value)

        query.requires_grad_(True)
        key.requires_grad_(True)
        value.requires_grad_(True)

        output = xops.memory_efficient_attention(query, key, value)
        loss = (output * query_grad).sum()
        loss.backward()

        print("‚úì Gradient computation successful"        print(f"  query.grad shape: {query.grad.shape if query.grad is not None else None}")
        print(f"  key.grad shape: {key.grad.shape if key.grad is not None else None}")
        print(f"  value.grad shape: {value.grad.shape if value.grad is not None else None}")

        return True

    except Exception as e:
        print(f"‚úó xFormers MPS test failed: {e}")
        traceback.print_exc()
        return False

def test_cpu_fallback():
    print("\n=== Testing CPU Fallback ===")
    try:
        import xformers.ops as xops

        # Create CPU tensors
        B, M, H, K = 2, 16, 4, 32  # Smaller for CPU
        query = torch.randn(B, M, H, K, dtype=torch.float32)
        key = torch.randn(B, M, H, K, dtype=torch.float32)
        value = torch.randn(B, M, H, K, dtype=torch.float32)

        print("Testing xFormers on CPU...")
        with torch.no_grad():
            output = xops.memory_efficient_attention(query, key, value)
        print(f"‚úì CPU attention successful: {output.shape}")

        return True

    except Exception as e:
        print(f"‚úó CPU fallback test failed: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print(f"PyTorch version: {torch.__version__}")
    print(f"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}")

    mps_success = test_mps_support()
    cpu_success = test_cpu_fallback()

    if mps_success or cpu_success:
        print("\nüéâ xFormers MPS implementation test completed successfully!")
    else:
        print("\n‚ùå All tests failed")
        sys.exit(1)
